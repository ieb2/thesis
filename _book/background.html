<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.36">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Jackknife Variance Estimator for Datasets Containing Multiply Imputed Outcome Variables Under Uncongeniality: A Monte Carlo Simulation Study - 2&nbsp; Background</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./methods.html" rel="next">
<link href="./abstract.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./"></a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span style="color:white">Title Page</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acknowledgments.html" class="sidebar-item-text sidebar-link">Acknowledgments</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./abstract.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Abstract</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./methods.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimator.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Proposed Jackknife Estimator</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./results.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Results</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discussion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Discussion</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">Appendix</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#bootstrap-resampling" id="toc-bootstrap-resampling" class="nav-link active" data-scroll-target="#bootstrap-resampling"> <span class="header-section-number">2.1</span> Bootstrap Resampling</a></li>
  <li><a href="#jackknife-resampling" id="toc-jackknife-resampling" class="nav-link" data-scroll-target="#jackknife-resampling"> <span class="header-section-number">2.2</span> Jackknife Resampling</a>
  <ul class="collapse">
  <li><a href="#sec-del" id="toc-sec-del" class="nav-link" data-scroll-target="#sec-del"> <span class="header-section-number">2.2.1</span> Leave-One-Out Jackknife</a></li>
  <li><a href="#delete-d-jackknife" id="toc-delete-d-jackknife" class="nav-link" data-scroll-target="#delete-d-jackknife"> <span class="header-section-number">2.2.2</span> Delete-<em>d</em> Jackknife</a></li>
  </ul></li>
  <li><a href="#comparison-of-jackknife-and-bootstrap-resampling" id="toc-comparison-of-jackknife-and-bootstrap-resampling" class="nav-link" data-scroll-target="#comparison-of-jackknife-and-bootstrap-resampling"> <span class="header-section-number">2.3</span> Comparison of Jackknife and Bootstrap Resampling</a></li>
  <li><a href="#multiple-imputation" id="toc-multiple-imputation" class="nav-link" data-scroll-target="#multiple-imputation"> <span class="header-section-number">2.4</span> Multiple Imputation</a>
  <ul class="collapse">
  <li><a href="#current-state-of-multiple-imputation" id="toc-current-state-of-multiple-imputation" class="nav-link" data-scroll-target="#current-state-of-multiple-imputation"> <span class="header-section-number">2.4.1</span> Current State of Multiple Imputation</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Missing data is a discipline-agnostic issue commonly encountered by statistical practitioners. Given that many statistical procedures require data to be complete (i.e., in the form of an <span class="math inline">\(n \times m\)</span> matrix), the appropriate course of action to be taken in the presence of missing data has long been investigated by statisticians. Today, multiple imputation is accepted as the gold standard in missing data analysis, thanks to the work of Donald Rubin <span class="citation" data-cites="buuren_flexible_2012"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a></span>. In 1977, Rubin proposed using multiple completed versions of the dataset with missing observations, applying the complete-data procedure of interest, and pooling the estimates to draw valid inferences <span class="citation" data-cites="buuren_flexible_2012"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a></span>. The main advantage of multiple imputation, as opposed to single imputation, which had been used by researchers since the early 1970s, is its ability to properly estimate the variance caused by missing observations <span class="citation" data-cites="buuren_flexible_2012"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a></span>. The emphasis placed on variance and uncertainty by Rubin was a departure from the status quo of the time, which was to fill in the missing observation with the most probable value and to proceed with complete case analyses as if the observation had not been missing, to begin with <span class="citation" data-cites="buuren_flexible_2012"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a></span>. This approach, however, fails to incorporate the loss of information caused by missing observations into the estimation of parameters, resulting in the underestimation of variance <span class="citation" data-cites="rubin1978multiple"><a href="references.html#ref-rubin1978multiple" role="doc-biblioref">[2]</a></span>.</p>
<p>Like all revolutionary ideas, multiple imputation received harsh criticism following its conception. Perhaps the most notable of the objections came from Fay in 1992, who demonstrated through counterexamples that multiple imputation produced biased covariance estimates <span class="citation" data-cites="bartlett_bootstrap_2020"><a href="references.html#ref-bartlett_bootstrap_2020" role="doc-biblioref">[3]</a></span>. Fay added that the need for unison between the imputation and analysis model made multiple imputation a poor general-purpose tool, particularly in instances where the imputer and analyst are different individuals <span class="citation" data-cites="fay1992inferences buuren_flexible_2012"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a>, <a href="references.html#ref-fay1992inferences" role="doc-biblioref">[4]</a></span>. Fay’s arguments led to the conceptualization of congeniality<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> between the imputation and analysis model, which was later accepted to be a requirement to obtain valid inferences from multiple imputation using Rubin’s pooling rules (hereafter, Rubin’s rules) <span class="citation" data-cites="buuren_flexible_2012 meng_multiple-imputation_1994"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a>, <a href="references.html#ref-meng_multiple-imputation_1994" role="doc-biblioref">[5]</a></span>. Briefly, uncongeniality refers to the imputation and analysis model making irreconcilably different assumptions regarding the data, which occurs rather frequently in practice <span class="citation" data-cites="bartlett_bootstrap_2020"><a href="references.html#ref-bartlett_bootstrap_2020" role="doc-biblioref">[3]</a></span>. Although Fay’s work initially criticized biases introduced to the covariance matrix following multiple imputation, a similar phenomenon of biased estimates were observed with variance estimators under uncongeniality <span class="citation" data-cites="fay1992inferences meng_multiple-imputation_1994 xie_dissecting_2016"><a href="references.html#ref-fay1992inferences" role="doc-biblioref">[4]</a>–<a href="references.html#ref-xie_dissecting_2016" role="doc-biblioref">[6]</a></span>.</p>
<p>Some of the earliest works demonstrating Rubin’s variance estimator to be biased under uncongeniality were from Wang and Robins in 1998, who also proposed an alternate variance estimator in the same paper <span class="citation" data-cites="buuren_flexible_2012"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a></span>. The variance estimator proposed by Wang and Robins requires the calculation of several quantities, which are not easily accessible to the average statistical practitioner <span class="citation" data-cites="bartlett_bootstrap_2020"><a href="references.html#ref-bartlett_bootstrap_2020" role="doc-biblioref">[3]</a></span>. The challenging construction of the variance estimator proposed by Wang and Robins has resulted in it receiving little-to-no attention in applied settings <span class="citation" data-cites="bartlett_bootstrap_2020"><a href="references.html#ref-bartlett_bootstrap_2020" role="doc-biblioref">[3]</a></span>. In an attempt to create a more user-friendly variance estimator in instances of suspected uncongeniality, researchers have proposed combining resampling methods with multiple imputation. Of the two main resampling methods, bootstrap has received more attention from multiple imputation researchers compared to jackknife resampling, which has mostly been investigated under single hot-deck imputation. Although particular combinations of bootstrap and multiple imputation have been demonstrated to create asymptotically unbiased estimates of variance, the associated computational cost makes this an active area of research <span class="citation" data-cites="bartlett_bootstrap_2020"><a href="references.html#ref-bartlett_bootstrap_2020" role="doc-biblioref">[3]</a></span>. Most recently, von Hippel has proposed a bootstrap variance estimator which addresses the issue of computational cost; however, it has been demonstrated to create confidence intervals that are slightly wider compared to traditional bootstrap and multiple imputation combinations <span class="citation" data-cites="bartlett_bootstrap_2020"><a href="references.html#ref-bartlett_bootstrap_2020" role="doc-biblioref">[3]</a></span>. Given the lower computational cost associated with jackknife resampling, as well as desirable properties demonstrated under single imputation, such as being unbiased in certain scenarios, it is an attractive alternative that should be considered as a variance estimator of multiply imputed data under uncongeniality <span class="citation" data-cites="chen_jackknife_2001 rao_jackknife_1992"><a href="references.html#ref-chen_jackknife_2001" role="doc-biblioref">[7]</a>, <a href="references.html#ref-rao_jackknife_1992" role="doc-biblioref">[8]</a></span>. More importantly, however, is the advantages jackknife resampling has over bootstrap resampling in small sample sizes, which are frequently encountered in datasets associated with biological studies.</p>
<section id="bootstrap-resampling" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="bootstrap-resampling"><span class="header-section-number">2.1</span> Bootstrap Resampling</h2>
<p>Let <span class="math inline">\(q\)</span> be the set of observations <span class="math inline">\(\left(z_1, z_2, z_3, \dots, z_n \right)\)</span> from the population <span class="math inline">\(Q\)</span> such that <span class="math inline">\(z_i \ \forall \ i \in \{1, 2, 3, \dots, n\}\)</span> is an i.i.d<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> sample from <span class="math inline">\(Q\)</span>. Moreover, let <span class="math inline">\(\theta\)</span> be some parameter of interest, with the unbiased estimator <span class="math inline">\(\hat{\theta}\)</span>, which is a statistic computed from <span class="math inline">\(F(q)\)</span>. Finally, let <span class="math inline">\(G_{\theta}\)</span> be the sampling distribution of <span class="math inline">\(F(q)\)</span>. The non-parametric bootstrap, as proposed by Efron, lets <span class="math inline">\(q\)</span> define <span class="math inline">\(Q\)</span> such that the set of observations <span class="math inline">\(\left(z_1, z_2, z_3, \dots, z_n \right)\)</span> appears with equal proportion in the infinitely large population. From there, the set of samples <span class="math inline">\(q^*_1, q^*_2, q^*_3, \dots, q^*_m\)</span> as <span class="math inline">\(m \rightarrow \infty\)</span> are generated by sampling <span class="math inline">\(n\)</span> observations with replacement from <span class="math inline">\(q\)</span>, and the statistic of interest <span class="math inline">\(\hat{\theta}\)</span> is calculated by applying <span class="math inline">\(F(q_i) \ \forall \ i \in \{1, 2, 3, \dots, m\}\)</span>, which results in <span class="math inline">\(\hat{\theta^*}_1, \hat{\theta^*}_2, \hat{\theta^*}_3, \dots, \hat{\theta^*}_m\)</span>.</p>
<p>Finally, the point estimate is obtained by</p>
<p><span class="math display">\[\hat{\theta} = m^{-1}\sum^m_{i = 1} \hat{\theta^*}_i\]</span></p>
<p>and the variance is obtained by</p>
<p><span class="math display">\[\text{var}(\hat{\theta}) = m^{-1}\sum^m_{i = 1} \left(\hat{\theta^*}_i - \hat{\theta}\right)^2\]</span></p>
<p>Since Efron’s proposal of the non-parametric bootstrap, statisticians have widely utilized it thanks to its ease of implementation and the rapidly increasing computational power available to statistical practitioners <span class="citation" data-cites="LaFontaine_2021"><a href="references.html#ref-LaFontaine_2021" role="doc-biblioref">[9]</a></span>. However, the properties of bootstrap resampling have their basis in the asymptotic theory, which holds in large sample sizes <span class="citation" data-cites="Wang_1998 Mammen_1992"><a href="references.html#ref-Wang_1998" role="doc-biblioref">[10]</a>, <a href="references.html#ref-Mammen_1992" role="doc-biblioref">[11]</a></span>. The minimum sample size required to utilize bootstrap resampling and obtain asymptotically unbiased estimates is highly context-dependent; in certain situations, a minimum sample size of <span class="math inline">\(n = 200\)</span> has been suggested, with other authors suggesting between <span class="math inline">\(n = 100\)</span> to <span class="math inline">\(n = 500\)</span> <span class="citation" data-cites="Anderson_Gerbing_1984 Bentler_Chou_1987 Jackson_2001"><a href="references.html#ref-Anderson_Gerbing_1984" role="doc-biblioref">[12]</a>–<a href="references.html#ref-Jackson_2001" role="doc-biblioref">[14]</a></span>. Given that in many biological studies, the minimum sample size required for asymptotically unbiased estimates may not be achieved, jackknife resampling, a method that predates the bootstrap, may be considered <span class="citation" data-cites="Faber_Fonseca_2014"><a href="references.html#ref-Faber_Fonseca_2014" role="doc-biblioref">[15]</a></span>.</p>
</section>
<section id="jackknife-resampling" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="jackknife-resampling"><span class="header-section-number">2.2</span> Jackknife Resampling</h2>
<section id="sec-del" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="sec-del"><span class="header-section-number">2.2.1</span> Leave-One-Out Jackknife</h3>
<p>Let <span class="math inline">\(q\)</span> be the set of observations <span class="math inline">\(\left(z_1, z_2, z_3, \dots, z_n \right)\)</span> from the population <span class="math inline">\(Q\)</span> such that <span class="math inline">\(z_i \ \forall \ i \in \{1, 2, 3, \dots, n\}\)</span> is an i.i.d sample from <span class="math inline">\(Q\)</span>. Moreover, let <span class="math inline">\(\theta\)</span> be some parameter of interest, with the unbiased estimator <span class="math inline">\(\hat{\theta}\)</span>, which is a statistic computed from <span class="math inline">\(F(q)\)</span>. Finally, let <span class="math inline">\(G_{\theta}\)</span> be the sampling distribution of <span class="math inline">\(F(q)\)</span>. The jackknife, as proposed by Quenouille and expanded on by Tukey, creates <span class="math inline">\(n\)</span> leave-one-out subsamples from <span class="math inline">\(q\)</span> such that <span class="math inline">\(q_{-1} = \left(z_2, z_3, z_4, \dots, z_n \right),\)</span> <span class="math inline">\(q_{-2} = \left(z_1, z_3, z_4, \dots, z_n \right), \dots, q_{-n} = \left(z_1, z_2, z_3, \dots, z_{n-1} \right)\)</span> and <span class="math inline">\(\lvert q_{-i}\rvert = n-1 \ \forall i \in \{1, 2, 3, \dots, n\}\)</span>. Thereafter, the statistic of interest <span class="math inline">\(\hat{\theta}\)</span> is calculated by applying <span class="math inline">\(F(q_i) \ \forall \ i \in \{1, 2, 3, \dots, n\}\)</span>, which results in <span class="math inline">\(\hat{\theta^*}_1, \hat{\theta^*}_2, \hat{\theta^*}_3, \dots, \hat{\theta^*}_m\)</span>.</p>
<p>Finally, the point estimate is obtained by</p>
<p><span class="math display">\[
\hat{\theta} = n^{-1}\sum^n_{i = 1} \hat{\theta^*}_i
\]</span></p>
<p>and the variance is obtained by</p>
<p><span class="math display">\[
\text{var}(\hat{\theta}) = n^{-1}\sum^n_{i = 1} \left(\hat{\theta^*}_i - \hat{\theta}\right)^2
\]</span></p>
</section>
<section id="delete-d-jackknife" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="delete-d-jackknife"><span class="header-section-number">2.2.2</span> Delete-<em>d</em> Jackknife</h3>
<p>The delete-<em>d</em> jackknife may be seen as a generalized version of the traditional jackknife (hereinafter, jackknife). Although in many situations, the jackknife provides a computationally efficient means to estimate the variance of an estimator, it fails with non-smooth statistics. For non-smooth statistics, such as the percentiles of the data, the jackknife fails because the statistic varies significantly between any two subsamples <span class="citation" data-cites="chen_jackknife_2001"><a href="references.html#ref-chen_jackknife_2001" role="doc-biblioref">[7]</a></span>. In such cases, the delete-<em>d</em> jackknife provides an alternative estimator that can provide asymptotically unbiased estimates for non-smooth statistics <span class="citation" data-cites="rao_jackknife_1992"><a href="references.html#ref-rao_jackknife_1992" role="doc-biblioref">[8]</a></span>. Similarly, let <span class="math inline">\(q\)</span> be the set of observations <span class="math inline">\(\left(z_1, z_2, z_3, \dots, z_n \right)\)</span> from the population <span class="math inline">\(Q\)</span> such that <span class="math inline">\(z_i \ \forall \ i \in \{1, 2, 3, \dots, n\}\)</span> is an i.i.d sample from <span class="math inline">\(Q\)</span>. Moreover, let <span class="math inline">\(\theta\)</span> be some parameter of interest, with the unbiased estimator <span class="math inline">\(\hat{\theta}\)</span>, which is a statistic computed from <span class="math inline">\(F(q)\)</span>. Finally, let <span class="math inline">\(G_{\theta}\)</span> be the sampling distribution of <span class="math inline">\(F(q)\)</span>. The delete-<em>d</em> jackknife, creates <span class="math inline">\(n \choose d\)</span> subsamples of <span class="math inline">\(q\)</span> such that <span class="math inline">\(\lvert q_i \rvert = n-d\)</span> Thereafter, the statistic of interest <span class="math inline">\(\hat{\theta}\)</span> is calculated by applying <span class="math inline">\(F(q_i) \ \forall \ i \in \{1, 2, 3, \dots, {n \choose d} \}\)</span>, which results in <span class="math inline">\(\hat{\theta^*}_1, \hat{\theta^*}_2, \hat{\theta^*}_3, \dots, \hat{\theta^*}_{n \choose d}\)</span>. In many cases, however, <span class="math inline">\(n \choose d\)</span> is a large value that yields this approach computationally unfeasible. In such instances, certain guidelines, as discussed in following chapters, may be employed to determine a value of subsamples that still provide proper estimates.</p>
<p>Finally, the point estimate is obtained by</p>
<p><span class="math display">\[
\hat{\theta} = {n \choose d}^{-1}\sum^{n \choose d}_{i = 1} \hat{\theta^*}_i
\]</span></p>
<p>and the variance is obtained by</p>
<p><span class="math display">\[
\text{var}(\hat{\theta}) = {n \choose d}^{-1}\sum^{n \choose d}_{i = 1} \left(\hat{\theta^*}_i - \hat{\theta}\right)^2
\]</span></p>
</section>
</section>
<section id="comparison-of-jackknife-and-bootstrap-resampling" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="comparison-of-jackknife-and-bootstrap-resampling"><span class="header-section-number">2.3</span> Comparison of Jackknife and Bootstrap Resampling</h2>
<p>Given their comparable construction and application, the jackknife and bootstrap make similar assumptions regarding the data. Most notably, both assume that the function used to estimate the parameter is smooth. Formally, a smooth function is defined as one that has continuous derivatives over some domain, with the minimum number of derivatives required to be considered smooth varying per the question at hand <span class="citation" data-cites="Weisstein"><a href="references.html#ref-Weisstein" role="doc-biblioref">[16]</a></span>. From a statistical perspective, if the function used to estimate the parameter is smooth on some domain <span class="math inline">\((a, b)\)</span>, <span class="math inline">\([F(q_i) - F(q_z)] \rightarrow 0\)</span> as <span class="math inline">\(\lvert q_i - q_z\rvert \rightarrow 0\)</span>. Meaning, among a set of conceivable, non-identical samples from the population, minor differences between possible samples will only result in minor differences between the statistic estimated <span class="citation" data-cites="chen_jackknife_2001"><a href="references.html#ref-chen_jackknife_2001" role="doc-biblioref">[7]</a></span>. Due to its deterministic nature, the jackknife tends to perform poorly in estimating non-smooth statistics <span class="citation" data-cites="wicklin_2017"><a href="references.html#ref-wicklin_2017" role="doc-biblioref">[17]</a></span>. However, its deterministic nature also yields the jackknife superior to bootstrapping in smaller datasets. To combat the issue regarding non-smooth statistics, a generalized jackknife resampling scheme, called the <em>drop-d-jackknife</em> has been proposed, which was utilized in this estimator <span class="citation" data-cites="chen_jackknife_2001"><a href="references.html#ref-chen_jackknife_2001" role="doc-biblioref">[7]</a></span>.</p>
</section>
<section id="multiple-imputation" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="multiple-imputation"><span class="header-section-number">2.4</span> Multiple Imputation</h2>
<p>Multiple imputation is a missing data management method based on both Bayesian and frequentist inference proposed by Donald Rubin in the late 1970s <span class="citation" data-cites="bartlett_bootstrap_2020 buuren_flexible_2012"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a>, <a href="references.html#ref-bartlett_bootstrap_2020" role="doc-biblioref">[3]</a></span>. Before Rubin’s proposal, statisticians had been utilizing various single-imputation methods and proceeding with the analyses of interest as if data had not been missing. However, Rubin noted that single-imputation methods could not accurately capture the uncertainty caused by the missing observations <span class="citation" data-cites="buuren_flexible_2012"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a></span>. In response, he proposed imputing any given datum with a series of plausible values from its posterior distribution, thus creating several complete versions of the observed dataset and applying the complete data analysis procedure to each generated dataset. He proposed a series of rules that could derive point and variance estimates that would adequately reflect the uncertainty caused by the missing observations.</p>
<p>Rubin’s idea to utilize multiple imputation was ridiculed at first, not only due to its drastically different interpretation of uncertainty but because of how unfeasible it was at the time <span class="citation" data-cites="buuren_flexible_2012"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a></span>. Rubin’s method would require statisticians to come up with an imputation model that would allow them to draw values from the posterior distribution. After that, they would have to draw several values and repeat the analysis multiple times with the numerous complete datasets. The preceding workflow was challenging to implement in an era of low computational power and expensive digital storage <span class="citation" data-cites="buuren_flexible_2012"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a></span>. As such, Rubin’s ideas did not receive immediate acceptance. However, since the late 1990s, with the increased access to computers and user-friendly statistical packages capable of implementing complex procedures, multiple imputations have been adopted and heavily researched, leading to various modified algorithms being utilized under different conditions to obtain valid inferences in the presence of missing data <span class="citation" data-cites="buuren_flexible_2012"><a href="references.html#ref-buuren_flexible_2012" role="doc-biblioref">[1]</a></span>. At this time, one of the most notable challenges remaining with multiple imputation is the concept of congeniality. Congeniality may be thought of as the imputation model and the analysis model making compatible assumptions regarding the data. In the early 1990s, Fay and Meng demonstrated that congeniality was required to obtain valid inferences from multiple imputation <span class="citation" data-cites="bartlett_bootstrap_2020 bartlett_reference-based_2021"><a href="references.html#ref-bartlett_bootstrap_2020" role="doc-biblioref">[3]</a>, <a href="references.html#ref-bartlett_reference-based_2021" role="doc-biblioref">[18]</a></span>.</p>
<section id="current-state-of-multiple-imputation" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="current-state-of-multiple-imputation"><span class="header-section-number">2.4.1</span> Current State of Multiple Imputation</h3>
<p>Today, with the advent of public databases, the imputer and the analyst may no longer be the same person. Even in the absence of such cases, the imputation and analysis model may still be uncongenial if there does not exist a unifying Bayesian model which embeds the imputer’s imputation model and the analyst’s complete data procedure <span class="citation" data-cites="bartlett_reference-based_2021"><a href="references.html#ref-bartlett_reference-based_2021" role="doc-biblioref">[18]</a></span>. As such, researchers have begun to develop approaches that combine resampling methods with multiple imputation to obtain valid inferences even under uncongeniality. Of the currently proposed methods, there exist two main limitations: a) the increased computational cost brought on by resampling methods alongside multiple imputation, and b) inference in smaller sample sizes. At this time, nearly all mainstream approaches proposed by researchers utilize bootstrap resampling and multiple imputation to obtain valid inferences under uncongeniality; however, as discussed above, bootstrap resampling requires a certain sample size to provide proper estimates. As a viable alternative to be utilized in instances where there is a small sample size, a jackknife variance estimator is proposed.</p>


<div id="refs" class="references csl-bib-body" role="doc-bibliography" style="display: none">
<div id="ref-buuren_flexible_2012" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">S. van Buuren, <em>Flexible imputation of missing data</em>. Boca Raton, <span>FL</span>: <span>CRC</span> Press, 2012.</div>
</div>
<div id="ref-rubin1978multiple" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">D. B. Rubin, <span>“Multiple imputations in sample surveys-a phenomenological bayesian approach to nonresponse,”</span> in <em>Proceedings of the survey research methods section of the american statistical association</em>, 1978, vol. 1, pp. 20–34.</div>
</div>
<div id="ref-bartlett_bootstrap_2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">J. W. Bartlett and R. A. Hughes, <span>“Bootstrap inference for multiple imputation under uncongeniality and misspecification,”</span> <em>Stat Methods Med Res</em>, vol. 29, no. 12, pp. 3533–3546, Dec. 2020, doi: <a href="https://doi.org/10.1177/0962280220932189">10.1177/0962280220932189</a>.</div>
</div>
<div id="ref-fay1992inferences" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">R. E. Fay, <span>“When are inferences from multiple imputation valid?.”</span> 1992.</div>
</div>
<div id="ref-meng_multiple-imputation_1994" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">X.-L. Meng, <span>“Multiple-imputation inferences with uncongenial sources of input,”</span> <em>Statistical Science</em>, vol. 9, no. 4, pp. 538–558, Nov. 1994, doi: <a href="https://doi.org/10.1214/ss/1177010269">10.1214/ss/1177010269</a>.</div>
</div>
<div id="ref-xie_dissecting_2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">X. Xie and X.-L. Meng, <span>“Dissecting multiple imputation from a multi-phase inference perspective: What happens when god’s, imputer’s and analyst’s models are uncongenial?”</span> <em><span>STAT</span> <span>SINICA</span></em>, 2016, doi: <a href="https://doi.org/10.5705/ss.2014.067">10.5705/ss.2014.067</a>.</div>
</div>
<div id="ref-chen_jackknife_2001" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">J. Chen and J. Shao, <span>“Jackknife variance estimation for nearest-neighbor imputation,”</span> <em>Journal of the American Statistical Association</em>, vol. 96, no. 453, pp. 260–269, Mar. 2001, doi: <a href="https://doi.org/10.1198/016214501750332839">10.1198/016214501750332839</a>.</div>
</div>
<div id="ref-rao_jackknife_1992" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">J. N. K. Rao and J. Shao, <span>“Jackknife variance estimation with survey data under hot deck imputation,”</span> <em>Biometrika</em>, vol. 79, no. 4, pp. 811–822, 1992, doi: <a href="https://doi.org/10.2307/2337236">10.2307/2337236</a>.</div>
</div>
<div id="ref-LaFontaine_2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">D. LaFontaine, <span>“The history of bootstrapping: Tracing the development of resampling with replacement,”</span> <em>The Mathematics Enthusiast</em>, vol. 18, no. 1–2, pp. 78–99, Jan. 2021, doi: <a href="https://doi.org/10.54870/1551-3440.1515">10.54870/1551-3440.1515</a>.</div>
</div>
<div id="ref-Wang_1998" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">N. Wang, <span>“Large-sample theory for parametric multiple imputation procedures,”</span> <em>Biometrika</em>, vol. 85, no. 4, pp. 935–948, Dec. 1998, doi: <a href="https://doi.org/10.1093/biomet/85.4.935">10.1093/biomet/85.4.935</a>.</div>
</div>
<div id="ref-Mammen_1992" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">E. Mammen, <em>When does bootstrap work?</em>, vol. 77. New York, NY: Springer New York, 1992. doi: <a href="https://doi.org/10.1007/978-1-4612-2950-6">10.1007/978-1-4612-2950-6</a>.</div>
</div>
<div id="ref-Anderson_Gerbing_1984" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">J. C. Anderson and D. W. Gerbing, <span>“The effect of sampling error on convergence, improper solutions, and goodness-of-fit indices for maximum likelihood confirmatory factor analysis,”</span> <em>Psychometrika</em>, vol. 49, no. 2, pp. 155–173, Jun. 1984, doi: <a href="https://doi.org/10.1007/BF02294170">10.1007/BF02294170</a>.</div>
</div>
<div id="ref-Bentler_Chou_1987" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">P. M. Bentler and C.-P. Chou, <span>“Practical issues in structural modeling,”</span> <em>Sociological Methods &amp; Research</em>, vol. 16, no. 1, pp. 78–117, Aug. 1987, doi: <a href="https://doi.org/10.1177/0049124187016001004">10.1177/0049124187016001004</a>.</div>
</div>
<div id="ref-Jackson_2001" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">D. L. Jackson, <span>“Sample size and number of parameter estimates in maximum likelihood confirmatory factor analysis: A monte carlo investigation,”</span> <em>Structural Equation Modeling: A Multidisciplinary Journal</em>, vol. 8, no. 2, pp. 205–223, Apr. 2001, doi: <a href="https://doi.org/10.1207/S15328007SEM0802_3">10.1207/S15328007SEM0802_3</a>.</div>
</div>
<div id="ref-Faber_Fonseca_2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">J. Faber and L. M. Fonseca, <span>“How sample size influences research outcomes,”</span> <em>Dental Press Journal of Orthodontics</em>, vol. 19, no. 4, pp. 27–29, 2014, doi: <a href="https://doi.org/10.1590/2176-9451.19.4.027-029.ebo">10.1590/2176-9451.19.4.027-029.ebo</a>.</div>
</div>
<div id="ref-Weisstein" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">E. W. Weisstein, <span>“Smooth function.”</span> Available: <a href="https://mathworld.wolfram.com/">https://mathworld.wolfram.com/</a></div>
</div>
<div id="ref-wicklin_2017" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">R. Wicklin, <span>“Jackknife estimates in SAS,”</span> <em>SAS Blog</em>. Jun. 2017. Available: <a href="https://blogs.sas.com/content/iml/2017/06/21/jackknife-estimate-standard-error-sas.html">https://blogs.sas.com/content/iml/2017/06/21/jackknife-estimate-standard-error-sas.html</a></div>
</div>
<div id="ref-bartlett_reference-based_2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">J. W. Bartlett, <span>“Reference-based multiple imputation—what is the right variance and how to estimate it,”</span> <em>Statistics in Biopharmaceutical Research</em>, pp. 1–9, Nov. 2021, doi: <a href="https://doi.org/10.1080/19466315.2021.1983455">10.1080/19466315.2021.1983455</a>.</div>
</div>
</div>
</section>
</section>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>Please see the appendix for a detailed overview of congeniality.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>i.i.d stands for independent and identically distributed, which is used to summarize two characteristics about the data; (1) that the samples are all taken from the same probability distribution, and (2) that the samples are obtained independently.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./abstract.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Abstract</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./methods.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>