[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jackknife Variance Estimator for Datasets Containing Multiply Imputed Outcome Variables Under Uncongeniality: A Monte Carlo Simulation Study",
    "section": "",
    "text": "A Thesis\n\n\nPresented to the Department of Mathematics and Statistics\n\n\nHal Marcus College of Science and Engineering\n\n\nand\n\n\nThe Kugelman Honors Program\n\n\nof\n\n\nThe University of West Florida\n\n\n\n\n\n\n\nIn partial fulfillment of the requirements for graduation as a Kugelman Honors Scholar\n\n\n\n\n\n\n\n\n\n\nIhsan E. Buker\n\n\nNovember, 2022\n\n\n\n\n\n\n\n\nWang, N. 1998. “Large-Sample Theory for Parametric Multiple Imputation Procedures.” Biometrika 85 (4): 935–48. https://doi.org/10.1093/biomet/85.4.935."
  },
  {
    "objectID": "acknowledgments.html",
    "href": "acknowledgments.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "Wang, N. 1998. “Large-Sample Theory for Parametric Multiple Imputation Procedures.” Biometrika 85 (4): 935–48. https://doi.org/10.1093/biomet/85.4.935."
  },
  {
    "objectID": "abstract.html",
    "href": "abstract.html",
    "title": "1  Abstract",
    "section": "",
    "text": "Missing data is an issue ubiquitous in many fields of science. Today, multiple imputation (MI) is one of the most commonly utilized approaches to provide valid statistical inferences in the presence of missing data. Briefly, MI fills the missing cells in the original dataset by generating a series of plausible values based on an imputation model and, thereafter, creates multiple complete versions of the original dataset. Subsequently, the analysis model is applied to each imputed dataset, and the parameters of interest are pooled to accurately reflect the loss of information caused by the missing observations. Accompanying MI, however, is the issue of uncongeniality, which, imprecisely, occurs when the imputation model and the analysis model make different assumptions about the data. Not long after the conception of MI, Rubin’s accompanying set of rules to pool parameter estimates from the multiply imputed datasets was shown to produce biased point estimates under uncongeniality, which led to under-coverage of confidence intervals for anti-conservative estimates of variance or over-coverage for conservative estimates. In response, certain combinations of MI and resampling methods have been proposed as robust variance estimators under uncongeniality; however, their main drawback, to this day, has been their associated computational cost. Moreover, bootstrapping, one of the most commonly utilized resampling methods alongside MI to obtain proper variance estimates, has its basis in asymptotic theory. As such, in small samples frequently encountered in biological studies, the need for a computationally efficient variance estimator with statistically desirable properties remains.\nIn response, a jackknife variance estimator for multiply imputed outcome variables under uncongeniality for small sample sizes is proposed, which provides asymptotically unbiased point estimates with appropriate confidence interval coverage under uncongeniality. The performance of the proposed jackknife variance estimator is investigated using a Monte Carlo simulation study and compared to other methods in the literature. Accordingly, the recommendation to replace Rubin’s rules as the de facto standard in variance estimation with resampling-based robust variance estimators is made, particularly in light of the modern computational power statistical practitioners have at their disposal. Finally, an implementation of the proposed jackknife variance estimator in R is provided."
  },
  {
    "objectID": "abstract.html#keywords",
    "href": "abstract.html#keywords",
    "title": "1  Abstract",
    "section": "1.1 Keywords",
    "text": "1.1 Keywords\nMultiple imputation, uncongeniality, model misspecification, jackknife resampling\n\n\n\n\nWang, N. 1998. “Large-Sample Theory for Parametric Multiple Imputation Procedures.” Biometrika 85 (4): 935–48. https://doi.org/10.1093/biomet/85.4.935."
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "2  Background",
    "section": "",
    "text": "Missing data is a discipline-agnostic issue commonly encountered by statistical practitioners. Given that many statistical procedures require data to be complete (i.e., in the form of an \\(n \\times m\\) matrix), the appropriate course of action to be taken in the presence of missing data has long been investigated by statisticians. Today, multiple imputation is accepted as the gold standard in missing data analysis, thanks to the work of Donald Rubin (Buuren 2012). In 1977, Rubin proposed using multiple completed versions of the dataset with missing observations, applying the complete-data procedure of interest, and pooling the estimates to draw valid inferences (Buuren 2012). The main advantage of multiple imputation, as opposed to single imputation, which had been used by researchers since the early 1970s, is its ability to properly estimate the variance caused by missing observations (Buuren 2012). The emphasis placed on variance and uncertainty by Rubin was a departure from the status quo of the time, which was to fill in the missing observation with the most probable value and to proceed with complete case analyses as if the observation had not been missing, to begin with (Buuren 2012). This approach, however, fails to incorporate the loss of information caused by missing observations into the estimation of parameters, resulting in the underestimation of variance (Rubin 1978).\nLike all revolutionary ideas, multiple imputation received harsh criticism following its conception. Perhaps the most notable of the objections came from Fay in 1992, who demonstrated through counterexamples that multiple imputation produced biased covariance estimates (Bartlett and Hughes 2020). Fay added that the need for unison between the imputation and analysis model made multiple imputation a poor general-purpose tool, particularly in instances where the imputer and analyst are different individuals (Fay 1992; Buuren 2012). Fay’s arguments led to the conceptualization of congeniality1 between the imputation and analysis model, which was later accepted to be a requirement to obtain valid inferences from multiple imputation using Rubin’s pooling rules (hereafter, Rubin’s rules) (Buuren 2012; Meng 1994). Although Fay’s work initially criticized biases introduced to the covariance matrix following multiple imputation, a similar phenomenon of biased estimations were observed with variance under uncongeniality (Fay 1992; Meng 1994; Xie and Meng 2016).\nSome of the earliest works demonstrating Rubin’s variance estimator to be biased under uncongeniality were from Wang and Robins in 1998, who also proposed an alternate variance estimator in the same paper (Buuren 2012). The variance estimator proposed by Wang and Robins requires the calculation of several quantities, which are not easily accessible to the average statistical practitioner (Bartlett and Hughes 2020). The challenging construction of the variance estimator proposed by Wang and Robins has resulted in it receiving little-to-no attention in applied settings (Bartlett and Hughes 2020). In an attempt to create a more user-friendly variance estimator in instances of suspected uncongeniality, researchers have proposed combining resampling methods with multiple imputation. Of the two main resampling methods, bootstrap has received more attention from multiple imputation researchers compared to jackknife resampling, which has mostly been investigated under single hot-deck imputation. Although particular combinations of bootstrap and multiple imputation have been demonstrated to create asymptotically unbiased estimates of variance, the associated computational cost makes this an active area of research (Bartlett and Hughes 2020). Most recently, von Hippel has proposed a bootstrap variance estimator which addresses the issue of computational cost; however, it has been demonstrated to create confidence intervals that are slightly wider compared to traditional bootstrap and multiple imputation combinations (Bartlett and Hughes 2020). Given the lower computational cost associated with jackknife resampling, as well as desirable properties demonstrated under single imputation, such as being unbiased in certain scenarios, it is an attractive alternative that should be considered as a variance estimator of multiply imputed data under uncongeniality (Chen and Shao 2001; Rao and Shao 1992). More importantly, however, is the advantages jackknife resampling has over bootstrap resampling in small sample sizes, which are frequently encountered in datasets associated with biological studies."
  },
  {
    "objectID": "background.html#bootstrap-resampling",
    "href": "background.html#bootstrap-resampling",
    "title": "2  Background",
    "section": "2.1 Bootstrap Resampling",
    "text": "2.1 Bootstrap Resampling\nLet \\(q\\) be the set of observations \\(\\left(z_1, z_2, z_3, \\dots, z_n \\right)\\) from the population \\(Q\\) such that \\(z_i \\ \\forall \\ i \\in \\{1, 2, 3, \\dots, n\\}\\) is an i.i.d sample from \\(Q\\). Moreover, let \\(\\theta\\) be some parameter of interest, with the unbiased estimator \\(\\hat{\\theta}\\), which is a statistic computed from \\(F(q)\\). Finally, let \\(G_{\\theta}\\) be the sampling distribution of \\(F(q)\\). The non-parametric bootstrap, as proposed by Efron, lets \\(q\\) define \\(Q\\) such that the set of observations \\(\\left(z_1, z_2, z_3, \\dots, z_n \\right)\\) appears with equal proportion in the infinitely large population. From there, the set of samples \\(q^*_1, q^*_2, q^*_3, \\dots, q^*_m\\) as \\(m \\rightarrow \\infty\\) are generated by sampling \\(n\\) observations with replacement from \\(q\\), and the statistic of interest \\(\\hat{\\theta}\\) is calculated by applying \\(F(q_i) \\ \\forall \\ i \\in \\{1, 2, 3, \\dots, m\\}\\), which results in \\(\\hat{\\theta^*}_1, \\hat{\\theta^*}_2, \\hat{\\theta^*}_3, \\dots, \\hat{\\theta^*}_m\\).\nFinally, the point estimate is obtained by\n\\[\\hat{\\theta} = m^{-1}\\sum^m_{i = 1} \\hat{\\theta^*}_i\\]\nand the variance is obtained by\n\\[\\text{var}(\\hat{\\theta}) = m^{-1}\\sum^m_{i = 1} \\left(\\hat{\\theta^*}_i - \\hat{\\theta}\\right)^2\\]\nSince Efron’s proposal of the non-parametric bootstrap, statisticians have widely utilized it thanks to its ease of implementation and the rapidly increasing computational power available to statistical practitioners. However, the properties of bootstrap resampling have their basis in the asymptotic theory, which holds in large sample sizes. The minimum sample size required to utilize bootstrap resampling and obtain asymptotically unbiased estimates is highly context-dependent; in certain situations, a minimum sample size of \\(n = 200\\) has been suggested, with other authors suggesting between \\(n = 100\\) to \\(n = 500\\). Given that in many biological studies, the minimum sample size required for asymptotically unbiased estimates may not be achieved, jackknife resampling, a method that predates the bootstrap, may be considered."
  },
  {
    "objectID": "background.html#jackknife-resampling",
    "href": "background.html#jackknife-resampling",
    "title": "2  Background",
    "section": "2.2 Jackknife Resampling",
    "text": "2.2 Jackknife Resampling\n\n2.2.1 Leave-One-Out Jackknife\nLet \\(q\\) be the set of observations \\(\\left(z_1, z_2, z_3, \\dots, z_n \\right)\\) from the population \\(Q\\) such that \\(z_i \\ \\forall \\ i \\in \\{1, 2, 3, \\dots, n\\}\\) is an i.i.d sample from \\(Q\\). Moreover, let \\(\\theta\\) be some parameter of interest, with the unbiased estimator \\(\\hat{\\theta}\\), which is a statistic computed from \\(F(q)\\). Finally, let \\(G_{\\theta}\\) be the sampling distribution of \\(F(q)\\). The jackknife, as proposed by Quenouille and expanded on by Tukey, creates \\(n\\) leave-one-out subsamples from \\(q\\) such that \\(q_{-1} = \\left(z_2, z_3, z_4, \\dots, z_n \\right),\\) \\(q_{-2} = \\left(z_1, z_3, z_4, \\dots, z_n \\right), \\dots, q_{-n} = \\left(z_1, z_2, z_3, \\dots, z_{n-1} \\right)\\) and \\(\\lvert q_{-i}\\rvert = n-1 \\ \\forall i \\in \\{1, 2, 3, \\dots, n\\}\\). Thereafter, the statistic of interest \\(\\hat{\\theta}\\) is calculated by applying \\(F(q_i) \\ \\forall \\ i \\in \\{1, 2, 3, \\dots, n\\}\\), which results in \\(\\hat{\\theta^*}_1, \\hat{\\theta^*}_2, \\hat{\\theta^*}_3, \\dots, \\hat{\\theta^*}_m\\).\nFinally, the point estimate is obtained by\n\\[\\hat{\\theta} = n^{-1}\\sum^n_{i = 1} \\hat{\\theta^*}_i\\]\nand the variance is obtained by\n\\[\\text{var}(\\hat{\\theta}) = n^{-1}\\sum^n_{i = 1} \\left(\\hat{\\theta^*}_i - \\hat{\\theta}\\right)^2\\]\n\n\n2.2.2 Delete-d Jackknife\nThe delete-d jackknife may be seen as a generalized version of the traditional jackknife (hereinafter, jackknife). Although in many situations, the jackknife provides a computationally efficient means to estimate the variance of an estimator, for non-smooth statistics, such as the percentiles of the data, the jackknife fails, as the statistic varies significantly between any two subsamples. In such cases, the delete-d jackknife provides an alternative estimator that can provide asymptotically unbiased estimates for non-smooth statistics. Similarly, let \\(q\\) be the set of observations \\(\\left(z_1, z_2, z_3, \\dots, z_n \\right)\\) from the population \\(Q\\) such that \\(z_i \\ \\forall \\ i \\in \\{1, 2, 3, \\dots, n\\}\\) is an i.i.d sample from \\(Q\\). Moreover, let \\(\\theta\\) be some parameter of interest, with the unbiased estimator \\(\\hat{\\theta}\\), which is a statistic computed from \\(F(q)\\). Finally, let \\(G_{\\theta}\\) be the sampling distribution of \\(F(q)\\). The delete-d jackknife, creates \\(n \\choose d\\) subsamples of \\(q\\) such that \\(\\lvert q_i \\rvert = n-d\\) Thereafter, the statistic of interest \\(\\hat{\\theta}\\) is calculated by applying \\(F(q_i) \\ \\forall \\ i \\in \\{1, 2, 3, \\dots, {n \\choose d} \\}\\), which results in \\(\\hat{\\theta^*}_1, \\hat{\\theta^*}_2, \\hat{\\theta^*}_3, \\dots, \\hat{\\theta^*}_{n \\choose d}\\). In many cases, however, \\(n \\choose d\\) is a large value that yields this approach computationally unfeasible. In such instances, certain guidelines, as discussed in following chapters, may be employed to determine a value of subsamples that still provide proper estimates.\nFinally, the point estimate is obtained by\n\\[\\hat{\\theta} = {n \\choose d}^{-1}\\sum^{n \\choose d}_{i = 1} \\hat{\\theta^*}_i\\]\nand the variance is obtained by\n\\[\\text{var}(\\hat{\\theta}) = {n \\choose d}^{-1}\\sum^{n \\choose d}_{i = 1} \\left(\\hat{\\theta^*}_i - \\hat{\\theta}\\right)^2\\]"
  },
  {
    "objectID": "background.html#comparison-of-jackknife-and-bootstrap-resampling",
    "href": "background.html#comparison-of-jackknife-and-bootstrap-resampling",
    "title": "2  Background",
    "section": "2.3 Comparison of Jackknife and Bootstrap Resampling",
    "text": "2.3 Comparison of Jackknife and Bootstrap Resampling\nGiven their comparable construction and application, the jackknife and bootstrap make similar assumptions regarding the data. Most notably, both assume that the function used to estimate the parameter is smooth. Formally, a smooth function is defined as one that has continuous derivatives over some domain, with the minimum number of derivatives required to be considered smooth varying per the question at hand. From a statistical perspective, if the function used to estimate the parameter is smooth on some domain \\((a, b)\\), \\([F(q_i) - F(q_z)] \\rightarrow 0\\) as \\(\\lvert q_i - q_z\\rvert \\rightarrow 0\\). Meaning, among a set of conceivable, non-identical samples from the population, minor differences between possible samples will only result in minor differences between the statistic estimated. Due to its deterministic nature, the jackknife tends to perform poorly in estimating non-smooth statistics. However, its deterministic nature also yields the jackknife superior to bootstrapping in smaller datasets. To combat the issue regarding non-smooth statistics, a generalized jackknife resampling scheme, called the drop-d-jackknife has been proposed, which was utilized in this estimator."
  },
  {
    "objectID": "background.html#multiple-imputation",
    "href": "background.html#multiple-imputation",
    "title": "2  Background",
    "section": "2.4 Multiple Imputation",
    "text": "2.4 Multiple Imputation\nMultiple imputation is a missing data management method based on both Bayesian and frequentist inference proposed by Donald Rubin in the late 1970s. Before Rubin’s proposal, statisticians had been utilizing various single-imputation methods and proceeding with the analyses of interest as if data had not been missing. However, Rubin noted that single-imputation methods could not accurately capture the uncertainty caused by the missing observations. In response, he proposed imputing any given datum with a series of plausible values from its posterior distribution, thus creating several complete versions of the observed dataset and applying the complete data analysis procedure to each generated dataset. He proposed a series of rules that could derive point and variance estimates that would adequately reflect the uncertainty caused by the missing observations.\nRubin’s idea to utilize multiple imputation was ridiculed at first, not only due to its drastically different interpretation of uncertainty but because of how unfeasible it was at the time. Rubin’s method would require statisticians to come up with an imputation model that would allow them to draw values from the posterior distribution. After that, they would have to draw several values and repeat the analysis multiple times with the numerous complete datasets. The preceding workflow was challenging to implement in an era of low computational power and expensive digital storage. As such, Rubin’s ideas did not receive immediate acceptance. However, since the late 1990s, with the increased access to computers and user-friendly statistical packages capable of implementing complex procedures, multiple imputations have been adopted and heavily researched, leading to various modified algorithms being utilized under different conditions to obtain valid inferences in the presence of missing data. At this time, one of the most notable challenges remaining with multiple imputation is the concept of congeniality. Congeniality may be thought of as the imputation model and the analysis model making compatible assumptions regarding the data. In the early 1990s, Fay and Meng demonstrated that congeniality was required to obtain valid inferences from multiple imputation.\n\n2.4.1 Current State of Multiple Imputation\nToday, with the advent of public databases, the imputer and the analyst may no longer be the same person. Even in the absence of such cases, the imputation and analysis model may still be uncongenial if there does not exist a unifying Bayesian model which embeds the imputer’s imputation model and the analyst’s complete data procedure. As such, researchers have begun to develop approaches that combine resampling methods with multiple imputation to obtain valid inferences even under uncongeniality. Of the currently proposed methods, there exist two main limitations: a) the increased computational cost brought on by resampling methods alongside multiple imputation, and b) inference in smaller sample sizes. At this time, nearly all mainstream approaches proposed by researchers utilize bootstrap resampling and multiple imputation to obtain valid inferences under uncongeniality; however, as discussed above, bootstrap resampling requires a certain sample size to provide proper estimates. As a viable alternative to be utilized in instances where there is a small sample size, a jackknife variance estimator is proposed.\n\n\n\n\nBartlett, Jonathan W, and Rachael A Hughes. 2020. “Bootstrap Inference for Multiple Imputation Under Uncongeniality and Misspecification.” Statistical Methods in Medical Research 29 (12): 3533–46. https://doi.org/10.1177/0962280220932189.\n\n\nBuuren, Stef van. 2012. Flexible Imputation of Missing Data. Chapman & Hall/CRC Interdisciplinary Statistics Series. Boca Raton, FL: CRC Press.\n\n\nChen, Jiahua, and Jun Shao. 2001. “Jackknife Variance Estimation for Nearest-Neighbor Imputation.” Journal of the American Statistical Association 96 (453): 260–69. https://doi.org/10.1198/016214501750332839.\n\n\nFay, Robert E. 1992. “When Are Inferences from Multiple Imputation Valid?.”\n\n\nMeng, Xiao-Li. 1994. “Multiple-Imputation Inferences with Uncongenial Sources of Input.” Statistical Science 9 (4): 538–58. https://doi.org/10.1214/ss/1177010269.\n\n\nRao, J. N. K., and J. Shao. 1992. “Jackknife Variance Estimation with Survey Data Under Hot Deck Imputation.” Biometrika 79 (4): 811–22. https://doi.org/10.2307/2337236.\n\n\nRubin, Donald B. 1978. “Multiple Imputations in Sample Surveys-a Phenomenological Bayesian Approach to Nonresponse.” In Proceedings of the Survey Research Methods Section of the American Statistical Association, 1:20–34. American Statistical Association Alexandria, VA, USA.\n\n\nWang, N. 1998. “Large-Sample Theory for Parametric Multiple Imputation Procedures.” Biometrika 85 (4): 935–48. https://doi.org/10.1093/biomet/85.4.935.\n\n\nXie, Xianchao, and Xiao-Li Meng. 2016. “Dissecting Multiple Imputation from a Multi-Phase Inference Perspective: What Happens When God’s, Imputer’s and Analyst’s Models Are Uncongenial?” Statistica Sinica. https://doi.org/10.5705/ss.2014.067."
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "3  Methods",
    "section": "",
    "text": "All generated datasets will be analysed using three approaches: * Bootstrap then multiply impute: The observed dataset with missing observations will initially be bootstrapped \\(n = 200\\) times. Thereafter, each of the bootstrap samples will be imputed \\(m = 2\\) times, with a maximum of \\(maxit = 5\\) iterations. The mean of the bootstrap estimates will serve as the final point estimate, and a 95% confidence interval will be generated through the percentile method, where the \\(\\alpha/2^{th}\\) and \\(1-(\\alpha/2)^{th}\\) percentiles will be the lower and upper bounds, respectively.\n\nMultiply impute then use Rubin’s rules: The observed dataset with missing observations will be imputed \\(m = 10\\) times, with a maximum of \\(maxit = 5\\) iterations. The point estimate, as well as the confidence interval will be obtained through the following rules proposed by Donald Rubin.\n\n\\[\\begin{align}\n\\bar{\\theta} &= \\frac{1}{m}\\left (\\sum_{i=1}^m{\\theta_i}\\right ) \\\\\n\nV_{within} &= \\frac{1}{m} \\sum_{i=1}^m{SE_i^2} \\\\\n\nV_{between} &= \\frac{\\sum_{i=1}^m (\\theta_i - \\overline{\\theta})^2}{m-1} \\\\\n\nV_{total} &= V_W + V_B + \\frac{V_B}{m} \\\\\n\n\\bar{\\theta} &\\pm t_{df,1-\\alpha/2} * \\sqrt{V_{total}} \\\\\n\\end{align}\\]\n\nJackknife then multiply impute: Altough a more detailed overview of the proposed jackknife estimator is provided in Chapter 4, briefly, the observed dataset will be jackknifed to obtain \\(j = 100\\) samples, each of which will be imputed \\(m = 2\\) times, with a maximum of \\(maxit = 5\\) iterations. The mean of the jackknife estimates will serve as the final point estimate, and a 95% confidence interval will be generated through the percentile method, where the \\(\\alpha/2^{th}\\) and \\(1-(\\alpha/2)^{th}\\) percentiles will be the lower and upper bounds, respectively.\n\nAll analyses will be conducted using 4.2.1 (Funny-Looking Kid).\n\n\n\n\nWang, N. 1998. “Large-Sample Theory for Parametric Multiple Imputation Procedures.” Biometrika 85 (4): 935–48. https://doi.org/10.1093/biomet/85.4.935."
  },
  {
    "objectID": "estimator.html",
    "href": "estimator.html",
    "title": "4  The Proposed Jackknife Estimator",
    "section": "",
    "text": "A pseudocode depiction of the proposed estimator.\n\n\nBriefly, the algorithm begins by obtaining \\(j\\) jackknife subsamples from the observed dataset with missing observations. Thereafter, each of the \\(j\\) subsamples are imputed \\(m\\) times, resulting in a total of \\(j \\times m\\) complete datasets. Subsequently, the analysis model of interest to estimate \\(\\theta\\) is applied to each of the completed datasets to produce \\(\\hat{\\theta^*}_1, \\hat{\\theta^*}_2, \\hat{\\theta^*}_3, \\dots, \\hat{\\theta^*}_{n \\choose d}\\). Finally, the vector of estimates is trimmed such that values outside of the \\(\\alpha^{th}\\) and \\(1-\\alpha^{th}\\) percentiles are excluded from the determination of the point estimate and bounds of the confidence interval.\nAs part of the algorithm, researchers must choose values \\(d\\) and \\(j\\), which will be context-dependent quantities. Ideally, a \\(d\\) value which satisfies \\(\\frac{\\sqrt{n}}{d} \\rightarrow 0\\) will provide asymptotically unbiased estimates even for non-smooth statistics. Rewriting the foregoing condition for \\(d\\):\n\\[\n\\begin{align}\n  &\\frac{\\sqrt{n}}{d} \\rightarrow \\ 0 \\\\\n  &\\implies d >> \\sqrt{n} \\\\\n  &\\text{Since} \\ d < n \\\\\n  &\\implies n > d >> \\sqrt{n}\n\\end{align}\n\\]\nIt is evident that \\(d\\) should take on some value between \\(n\\) and \\(\\sqrt{n}\\) with \\(d\\) being closer to \\(n\\), particulary for non-smooth statistics. At any rate, \\(j = \\binom{n}{d}\\) will likely be a value that is not computationally feasible to obtain. Again, the number of subsamples required, \\(j\\), can be limited to yield the estimator more accessible. The choice of \\(j\\) will be a multifaceted decision, where, if possible, greater values are preferred. Ideally, a small pilot study may be performed with a range of \\(j\\) values to determine values of \\(j\\) for which estimates begin to converge.\n\n\n\n\nWang, N. 1998. “Large-Sample Theory for Parametric Multiple Imputation Procedures.” Biometrika 85 (4): 935–48. https://doi.org/10.1093/biomet/85.4.935."
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "5  Results",
    "section": "",
    "text": "Per the Methods section, the performance of the proposed jackknife estimator was compared to two leading methods in the literature, Rubin’s rules following multiple imputation and Bootstrap resampling prior to multiple imputation. The methods were compared concerning the coverage probabilities they generated, the widths of their respective confidence intervals, their computational expense, and the bias of their point estimators."
  },
  {
    "objectID": "results.html#point-estimates",
    "href": "results.html#point-estimates",
    "title": "5  Results",
    "section": "5.1 Point Estimates",
    "text": "5.1 Point Estimates\nAll methods, perhaps with the exception of Rubin’s rules, produced reasonable point estimates with minimal bias. Rubin’s rules resulted in slightly anticonservative point estimates with greater standard deviation, indicative of a statistically inefficient estimator with high variability. This finding is unsurprising given the literature on Rubin’s rules and its performance under uncongeniality.\nIn contrast, it is noted that both resampling methods examined provide nearly unbiased point estimates with smaller standard deviations compared to Rubin’s rules. Again, given the literature on uncongeniality, this finding was unsurprising; however, given the lack of both empirical and theoretical justification for the bootstrap approach in small sample sizes, the desirable properties noted are worthy of further examination. Between all three methods, nevertheless, it is noted that the proposed jackknife estimator produced estimates with the smallest amount of bias, as well as the smallest standard deviation, an observation perhaps better observed in Section 5.1.2, where the distribution of the biases of the point estimates is compared. From the kernel density plots presented, it is evident that Rubin’s rules slightly underestimate the parameter. Compared to Rubin’s rules, both resampling approaches provide point estimates that are nearly unbiased; however, it is noted that the bootstrap estimates, despite being centered near zero, are slightly more dispersed compared to the jackknife estimates, which present as a narrow distribution centered at zero. This observation is justified by the values provided in Section 5.1.1 which demonstrate that the jackknife point estimates have a smaller standard deviation than the bootstrap approach and Rubin’s rules.\nFrom a point estimation perspective, the statistically desirable properties of the jackknife estimator, alongside its theoretical and empirical justification when used with small sample sizes, yield it a desirable estimator.\n\n5.1.1 Summary of Point Estimation Properties\n\n\n\n\n  \n\n\n\n\n\n5.1.2 Distribution of Point Estimator Bias\n\n\nCode\npoint_dist"
  },
  {
    "objectID": "results.html#sec-ci",
    "href": "results.html#sec-ci",
    "title": "5  Results",
    "section": "5.2 Confidence Intervals",
    "text": "5.2 Confidence Intervals\nOver-coverage of confidence intervals is noted across all methods; however, particulary with the jackknife and bootstrap approaches, such over-coverage is only slightly over the nominal, as such, they are likely not of concern and can be explained, in part, by the Monte Carlo standard error for the true coverage proabability of a 95% confidence interval. Among the three methods noted, Rubin’s rules, by a significant margin, deviates from the nominal coverage, indicative of an overly conservative estimator. An argument could be made, particulary in the case of biological studies that an overly conservative estimator is safer than one that is anti-conservative; however, the statistical inefficieny created by conservative estimators can be of concern, particulary in instances where small sample sizes are present or the test being utilized already has low statistical power. Comparing these methods concerning confidence interval width, it is noted that both resampling approaches provide narrower confidence intervals, with the jackknife approach providing the narrowest confidence intervals by a wide margin. In instances where nominal, or near-nominal coverage is reached, narrower confidence intervals are indicative of more efficient estimators. Given the near-nominal coverage noted with the jackknife estimator, combined with the narrow confidence intervals it produces, its superity to the two other methods may be inferred.\nA visual overview of the coverage probabilities may be noted in Section 5.2.1, where zipper plots of the methods are presented utilizing a simple random sample of 100 observations from the Monte Carlo simulation results of all methods examined. Given the small number of subsamples examined for visual clarity, the plots may not be indicative of the larger results presented above; however, they provide an appreciation for the meaning of coverage probabilities.\n\n\n\n\n  \n\n\n\n\n5.2.1 Zipper Plots for Confidence Interval Coverage\n\n\nCode\nggarrange(jackk, boot, rubin, ncol = 1, nrow = 3, common.legend = TRUE, legend=\"right\")"
  },
  {
    "objectID": "results.html#performance-benchmark-results",
    "href": "results.html#performance-benchmark-results",
    "title": "5  Results",
    "section": "5.3 Performance Benchmark Results",
    "text": "5.3 Performance Benchmark Results\nFinally, the three methods are compared concerning their computational expenses. Comparing the two approaches, which require further resampling, it is noted that the bootstrap approach takes nearly ten times longer than the jackknife approach per iteration. Unsurprisingly, Rubin’s rules, which do not rely on any further resampling besides the one performed during multiple imputation, was the fastest approach. However, given its biased estimates under uncongeniality or misspecification, the computational advantage it brings to the table adds very little value.\nDespite generating the same number of subsamples \\((n = 200)\\), albeit in contrasting manners, with the same number of imputations \\((m = 2)\\) and iterations \\((maxit = 5)\\), it is surprising that the bootstrap approach took nearly ten times longer per iterations to provide estimates. Regardless, the significantly reduced computational expense of the jackknife estimator yields it superior to the bootstrap approach under this particular scenario.\n\n\n\n\n  \n\n\n\n\n\nCode\nbenchmark_plot\n\n\n\n\n\n\n\n\n\nWang, N. 1998. “Large-Sample Theory for Parametric Multiple Imputation Procedures.” Biometrika 85 (4): 935–48. https://doi.org/10.1093/biomet/85.4.935."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bartlett, Jonathan W, and Rachael A Hughes. 2020. “Bootstrap\nInference for Multiple Imputation Under Uncongeniality and\nMisspecification.” Statistical Methods in Medical\nResearch 29 (12): 3533–46. https://doi.org/10.1177/0962280220932189.\n\n\nBuuren, Stef van. 2012. Flexible Imputation of Missing Data.\nChapman & Hall/CRC Interdisciplinary Statistics Series.\nBoca Raton, FL: CRC Press.\n\n\nChen, Jiahua, and Jun Shao. 2001. “Jackknife Variance Estimation\nfor Nearest-Neighbor Imputation.” Journal of the American\nStatistical Association 96 (453): 260–69. https://doi.org/10.1198/016214501750332839.\n\n\nFay, Robert E. 1992. “When Are Inferences from Multiple Imputation\nValid?.”\n\n\nMeng, Xiao-Li. 1994. “Multiple-Imputation Inferences with\nUncongenial Sources of Input.” Statistical Science 9\n(4): 538–58. https://doi.org/10.1214/ss/1177010269.\n\n\nRao, J. N. K., and J. Shao. 1992. “Jackknife Variance Estimation\nwith Survey Data Under Hot Deck Imputation.” Biometrika\n79 (4): 811–22. https://doi.org/10.2307/2337236.\n\n\nRubin, Donald B. 1978. “Multiple Imputations in Sample Surveys-a\nPhenomenological Bayesian Approach to Nonresponse.” In\nProceedings of the Survey Research Methods Section of the American\nStatistical Association, 1:20–34. American Statistical Association\nAlexandria, VA, USA.\n\n\nXie, Xianchao, and Xiao-Li Meng. 2016. “Dissecting Multiple\nImputation from a Multi-Phase Inference Perspective: What Happens When\nGod’s, Imputer’s and Analyst’s Models Are Uncongenial?”\nStatistica Sinica. https://doi.org/10.5705/ss.2014.067."
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "\\(N\\) is the total number of units in the finite population being targeted.\n\n\\(X\\) is an \\(N \\times q\\) matrix of fully observed covariates.\n\n\\(Y\\) is an \\(N \\times p\\) matrix of partially observed outcome variables.\n\n\\(R\\) is an \\(N \\times p\\) matrix of response indicators (i.e., \\(R_{ij} = 1\\) if the response on \\(Y_{ij}\\) is obtained and \\(R_{ij} = 0\\) otherwise.)\n\n\\(Q\\) is an unknown quantity of interest to the analyst.\n\\(Z_c = \\{X, Y_{inc}\\}\\) is the complete data.\n\\(Z_o = \\{X, Y_{obs}, R_{inc}\\}\\) is the incomplete (i.e., observed) data.\nThe analyst’s complete-data procedure is summarized by \\(\\mathscr{P}_{com} = [\\hat{Q}(X, Y_{inc}), U(X, Y_{inc})]\\), where \\(\\hat{Q}(X, Y_{inc})\\) is an estimator of \\(Q\\) with associated variance \\(U(X, Y_{inc}.)\\)\n\\(R\\) is not a part of \\(\\mathscr{P}_{com}\\), as the missing at random assumption implies that the response behavior itself carries no information about \\(Q\\)."
  },
  {
    "objectID": "appendix.html#formal",
    "href": "appendix.html#formal",
    "title": "Appendix",
    "section": "Formal Definition of Congeniality",
    "text": "Formal Definition of Congeniality\nIn short, one may define congeniality as the imputer and analyst making different assumptions regarding the data. The following two-part formal definition of uncongeniality was proposed by Meng in 1994, and will be utilized in our research. Meeting the assumptions set forth in the following two definitions qualifies the imputation model as being congenial to the analysis model, or vice versa.\n\nDefinition 1\nLet \\(E_f\\) and \\(V_f\\) denote posterior mean and variance with respect to \\(f\\), respectively. A Bayesian model \\(f\\) is said to be congenial to the analysis procedure \\(\\mathscr{P} \\equiv \\{\\mathscr{P}_{obs}; \\mathscr{P}_{com}\\}\\) for given \\(Z_o\\) if the following hold:\n\nThe posterior mean and variance of \\(\\theta\\) under \\(f\\) given the incomplete data are asymptotically the same as the estimate and variance from the analyst’s incomplete-data procedure \\(\\mathscr{P}_{obs}\\), that is, \\[\\begin{equation}\n    [\\hat{\\theta}(Z_o), U(Z_o)] \\simeq [E_f[\\theta | Z_o], V_f[\\theta | Z_o]]\n\\end{equation}\\]\nThe posterior mean and variance of \\(\\theta\\) under \\(f\\) given the complete data are asymptotically the same as the estimate and variance from the analyst’s complete-data procedure \\(\\mathscr{P}_{com}\\), that is, \\[\\begin{equation}\n     [\\hat{\\theta}(Z_c), U(Z_c)] \\simeq [E_f[\\theta | Z_c], V_f[\\theta | Z_c]]\n\\end{equation}\\]\nfor any possible \\(Y_{inc} = (Y_{obs}, Y_{miss})\\) with \\(Y_{obs}\\) conditioned upon.\n\nIf the foregoing conditions are met, \\(f\\) is said to be second-moment congenial to \\(\\mathscr{P}\\).\n\n\nDefinition 2\nThe analysis procedure \\(\\mathscr{P}\\) is said to be congenial to the imputation model \\(g(Y_{miss}|Z_o, A)\\) where \\(A\\) represents possible additional data the imputer has access to, if one can find an \\(f\\) such that (i) \\(f\\) is congenial to \\(\\mathscr{P}\\) and (ii) the posterior predictive density for \\(Y_{miss}\\) derived under \\(f\\) is identical to the imputation model \\(f(Y_{miss}|Z_o) = g(Y_{miss}|Z_o, A) \\ \\forall \\ Y_{miss}\\)."
  }
]