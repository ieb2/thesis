% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Jackknife Variance Estimator for Datasets Containing Multiply Imputed Outcome Variables Under Uncongeniality: A Monte Carlo Simulation Study},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Jackknife Variance Estimator for Datasets Containing Multiply
Imputed Outcome Variables Under Uncongeniality: A Monte Carlo Simulation
Study}
\author{}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[enhanced, frame hidden, interior hidden, borderline west={3pt}{0pt}{shadecolor}, breakable, sharp corners, boxrule=0pt]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{title-page}{%
\chapter*{\texorpdfstring{{Title Page}}{Title Page}}\label{title-page}}

A Thesis

Presented to the Department of Mathematics and Statistics

Hal Marcus College of Science and Engineering

and

The Kugelman Honors Program

of

The University of West Florida

In partial fulfillment of the requirements for graduation as a Kugelman
Honors Scholar

Ihsan E. Buker

November, 2022

\bookmarksetup{startatroot}

\hypertarget{acknowledgments}{%
\chapter*{Acknowledgments}\label{acknowledgments}}

I would like to thank my cat.

\bookmarksetup{startatroot}

\hypertarget{abstract}{%
\chapter{Abstract}\label{abstract}}

Missing data is an issue ubiquitous in many fields of science. Today,
multiple imputation (MI) is one of the most commonly utilized approaches
to provide valid statistical inferences in the presence of missing data.
Briefly, MI fills the missing cells in the original dataset by
generating a series of plausible values based on an imputation model
and, thereafter, creates multiple complete versions of the original
dataset. Subsequently, the analysis model is applied to each imputed
dataset, and the parameters of interest are pooled to accurately reflect
the loss of information caused by the missing observations. Accompanying
MI is the issue of uncongeniality, which occurs when the imputation
model and the analysis model make different assumptions about the data.
Not long after the conception of MI, Rubin's accompanying set of rules
to pool parameter estimates from the multiply imputed datasets was shown
to produce biased point estimates under uncongeniality, which led to
under-coverage of confidence intervals for anti-conservative estimates
of variance or over-coverage for conservative estimates. In response,
certain combinations of MI and resampling methods have been proposed as
robust variance estimators under uncongeniality; however, their main
drawback, to this day, has been their associated computational cost.
Moreover, bootstrapping, one of the most commonly utilized resampling
methods alongside MI to obtain proper variance estimates, has its basis
in asymptotic theory. As such, in small samples frequently encountered
in biological studies, the need for a computationally efficient variance
estimator with statistically desirable properties remains.

In response, a jackknife variance estimator for multiply imputed outcome
variables under uncongeniality for small sample sizes is proposed, which
provides asymptotically unbiased point estimates with appropriate
confidence interval coverage under uncongeniality. The performance of
the proposed jackknife variance estimator is investigated using a Monte
Carlo simulation study and compared to other methods in the literature.
Accordingly, the recommendation to replace Rubin's rules as the de facto
standard in variance estimation with resampling-based robust variance
estimators is made, particularly in light of the modern computational
power statistical practitioners have at their disposal. Finally, an
implementation of the proposed jackknife variance estimator in R is
provided.

\hypertarget{keywords}{%
\section{Keywords}\label{keywords}}

Multiple imputation, uncongeniality, model misspecification, jackknife
resampling

\bookmarksetup{startatroot}

\hypertarget{background}{%
\chapter{Background}\label{background}}

Missing data is a discipline-agnostic issue commonly encountered by
statistical practitioners. Given that many statistical procedures
require data to be complete (i.e., in the form of an \(n \times m\)
matrix), the appropriate course of action to be taken in the presence of
missing data has long been investigated by statisticians. Today,
multiple imputation is accepted as the gold standard in missing data
analysis, thanks to the work of Donald Rubin (Buuren 2012). In 1977,
Rubin proposed using multiple completed versions of the dataset with
missing observations, applying the complete-data procedure of interest,
and pooling the estimates to draw valid inferences (Buuren 2012). The
main advantage of multiple imputation, as opposed to single imputation,
which had been used by researchers since the early 1970s, is its ability
to properly estimate the variance caused by missing observations (Buuren
2012). The emphasis placed on variance and uncertainty by Rubin was a
departure from the status quo of the time, which was to fill in the
missing observation with the most probable value and to proceed with
complete case analyses as if the observation had not been missing, to
begin with (Buuren 2012). This approach, however, fails to incorporate
the loss of information caused by missing observations into the
estimation of parameters, resulting in the underestimation of variance
(Rubin 1978).

Like all revolutionary ideas, multiple imputation received harsh
criticism following its conception. Perhaps the most notable of the
objections came from Fay in 1992, who demonstrated through
counterexamples that multiple imputation produced biased covariance
estimates (Jonathan W. Bartlett and Hughes 2020). Fay added that the
need for unison between the imputation and analysis model made multiple
imputation a poor general-purpose tool, particularly in instances where
the imputer and analyst are different individuals (Fay 1992; Buuren
2012). Fay's arguments led to the conceptualization of
congeniality\footnote{Please see the appendix for a detailed overview of
  congeniality.} between the imputation and analysis model, which was
later accepted to be a requirement to obtain valid inferences from
multiple imputation using Rubin's pooling rules (hereafter, Rubin's
rules) (Buuren 2012; Meng 1994). Although Fay's work initially
criticized biases introduced to the covariance matrix following multiple
imputation, a similar phenomenon of biased estimations were observed
with variance under uncongeniality (Fay 1992; Meng 1994; Xie and Meng
2016).

Some of the earliest works demonstrating Rubin's variance estimator to
be biased under uncongeniality were from Wang and Robins in 1998, who
also proposed an alternate variance estimator in the same paper (Buuren
2012). The variance estimator proposed by Wang and Robins requires the
calculation of several quantities, which are not easily accessible to
the average statistical practitioner (Jonathan W. Bartlett and Hughes
2020). The challenging construction of the variance estimator proposed
by Wang and Robins has resulted in it receiving little-to-no attention
in applied settings (Jonathan W. Bartlett and Hughes 2020). In an
attempt to create a more user-friendly variance estimator in instances
of suspected uncongeniality, researchers have proposed combining
resampling methods with multiple imputation. Of the two main resampling
methods, bootstrap has received more attention from multiple imputation
researchers compared to jackknife resampling, which has mostly been
investigated under single hot-deck imputation. Although particular
combinations of bootstrap and multiple imputation have been demonstrated
to create asymptotically unbiased estimates of variance, the associated
computational cost makes this an active area of research (Jonathan W.
Bartlett and Hughes 2020). Most recently, von Hippel has proposed a
bootstrap variance estimator which addresses the issue of computational
cost; however, it has been demonstrated to create confidence intervals
that are slightly wider compared to traditional bootstrap and multiple
imputation combinations (Jonathan W. Bartlett and Hughes 2020). Given
the lower computational cost associated with jackknife resampling, as
well as desirable properties demonstrated under single imputation, such
as being unbiased in certain scenarios, it is an attractive alternative
that should be considered as a variance estimator of multiply imputed
data under uncongeniality (Chen and Shao 2001; Rao and Shao 1992). More
importantly, however, is the advantages jackknife resampling has over
bootstrap resampling in small sample sizes, which are frequently
encountered in datasets associated with biological studies.

\hypertarget{bootstrap-resampling}{%
\section{Bootstrap Resampling}\label{bootstrap-resampling}}

Let \(q\) be the set of observations
\(\left(z_1, z_2, z_3, \dots, z_n \right)\) from the population \(Q\)
such that \(z_i \ \forall \ i \in \{1, 2, 3, \dots, n\}\) is an i.i.d
sample from \(Q\). Moreover, let \(\theta\) be some parameter of
interest, with the unbiased estimator \(\hat{\theta}\), which is a
statistic computed from \(F(q)\). Finally, let \(G_{\theta}\) be the
sampling distribution of \(F(q)\). The non-parametric bootstrap, as
proposed by Efron, lets \(q\) define \(Q\) such that the set of
observations \(\left(z_1, z_2, z_3, \dots, z_n \right)\) appears with
equal proportion in the infinitely large population. From there, the set
of samples \(q^*_1, q^*_2, q^*_3, \dots, q^*_m\) as
\(m \rightarrow \infty\) are generated by sampling \(n\) observations
with replacement from \(q\), and the statistic of interest
\(\hat{\theta}\) is calculated by applying
\(F(q_i) \ \forall \ i \in \{1, 2, 3, \dots, m\}\), which results in
\(\hat{\theta^*}_1, \hat{\theta^*}_2, \hat{\theta^*}_3, \dots, \hat{\theta^*}_m\).

Finally, the point estimate is obtained by

\[\hat{\theta} = m^{-1}\sum^m_{i = 1} \hat{\theta^*}_i\]

and the variance is obtained by

\[\text{var}(\hat{\theta}) = m^{-1}\sum^m_{i = 1} \left(\hat{\theta^*}_i - \hat{\theta}\right)^2\]

Since Efron's proposal of the non-parametric bootstrap, statisticians
have widely utilized it thanks to its ease of implementation and the
rapidly increasing computational power available to statistical
practitioners (LaFontaine 2021). However, the properties of bootstrap
resampling have their basis in the asymptotic theory, which holds in
large sample sizes (Wang 1998; Mammen 1992). The minimum sample size
required to utilize bootstrap resampling and obtain asymptotically
unbiased estimates is highly context-dependent; in certain situations, a
minimum sample size of \(n = 200\) has been suggested, with other
authors suggesting between \(n = 100\) to \(n = 500\) (Anderson and
Gerbing 1984; Bentler and Chou 1987; Jackson 2001). Given that in many
biological studies, the minimum sample size required for asymptotically
unbiased estimates may not be achieved, jackknife resampling, a method
that predates the bootstrap, may be considered (Faber and Fonseca 2014).

\hypertarget{jackknife-resampling}{%
\section{Jackknife Resampling}\label{jackknife-resampling}}

\hypertarget{sec-del}{%
\subsection{Leave-One-Out Jackknife}\label{sec-del}}

Let \(q\) be the set of observations
\(\left(z_1, z_2, z_3, \dots, z_n \right)\) from the population \(Q\)
such that \(z_i \ \forall \ i \in \{1, 2, 3, \dots, n\}\) is an i.i.d
sample from \(Q\). Moreover, let \(\theta\) be some parameter of
interest, with the unbiased estimator \(\hat{\theta}\), which is a
statistic computed from \(F(q)\). Finally, let \(G_{\theta}\) be the
sampling distribution of \(F(q)\). The jackknife, as proposed by
Quenouille and expanded on by Tukey, creates \(n\) leave-one-out
subsamples from \(q\) such that
\(q_{-1} = \left(z_2, z_3, z_4, \dots, z_n \right),\)
\(q_{-2} = \left(z_1, z_3, z_4, \dots, z_n \right), \dots, q_{-n} = \left(z_1, z_2, z_3, \dots, z_{n-1} \right)\)
and \(\lvert q_{-i}\rvert = n-1 \ \forall i \in \{1, 2, 3, \dots, n\}\).
Thereafter, the statistic of interest \(\hat{\theta}\) is calculated by
applying \(F(q_i) \ \forall \ i \in \{1, 2, 3, \dots, n\}\), which
results in
\(\hat{\theta^*}_1, \hat{\theta^*}_2, \hat{\theta^*}_3, \dots, \hat{\theta^*}_m\).

Finally, the point estimate is obtained by

\[
\hat{\theta} = n^{-1}\sum^n_{i = 1} \hat{\theta^*}_i
\]

and the variance is obtained by

\[
\text{var}(\hat{\theta}) = n^{-1}\sum^n_{i = 1} \left(\hat{\theta^*}_i - \hat{\theta}\right)^2
\]

\hypertarget{delete-d-jackknife}{%
\subsection{\texorpdfstring{Delete-\emph{d}
Jackknife}{Delete-d Jackknife}}\label{delete-d-jackknife}}

The delete-\emph{d} jackknife may be seen as a generalized version of
the traditional jackknife (hereinafter, jackknife). Although in many
situations, the jackknife provides a computationally efficient means to
estimate the variance of an estimator, for non-smooth statistics, such
as the percentiles of the data, the jackknife fails, as the statistic
varies significantly between any two subsamples (Chen and Shao 2001). In
such cases, the delete-\emph{d} jackknife provides an alternative
estimator that can provide asymptotically unbiased estimates for
non-smooth statistics (Rao and Shao 1992). Similarly, let \(q\) be the
set of observations \(\left(z_1, z_2, z_3, \dots, z_n \right)\) from the
population \(Q\) such that
\(z_i \ \forall \ i \in \{1, 2, 3, \dots, n\}\) is an i.i.d sample from
\(Q\). Moreover, let \(\theta\) be some parameter of interest, with the
unbiased estimator \(\hat{\theta}\), which is a statistic computed from
\(F(q)\). Finally, let \(G_{\theta}\) be the sampling distribution of
\(F(q)\). The delete-\emph{d} jackknife, creates \(n \choose d\)
subsamples of \(q\) such that \(\lvert q_i \rvert = n-d\) Thereafter,
the statistic of interest \(\hat{\theta}\) is calculated by applying
\(F(q_i) \ \forall \ i \in \{1, 2, 3, \dots, {n \choose d} \}\), which
results in
\(\hat{\theta^*}_1, \hat{\theta^*}_2, \hat{\theta^*}_3, \dots, \hat{\theta^*}_{n \choose d}\).
In many cases, however, \(n \choose d\) is a large value that yields
this approach computationally unfeasible. In such instances, certain
guidelines, as discussed in following chapters, may be employed to
determine a value of subsamples that still provide proper estimates.

Finally, the point estimate is obtained by

\[ 
\hat{\theta} = {n \choose d}^{-1}\sum^{n \choose d}_{i = 1} \hat{\theta^*}_i
\]

and the variance is obtained by

\[
\text{var}(\hat{\theta}) = {n \choose d}^{-1}\sum^{n \choose d}_{i = 1} \left(\hat{\theta^*}_i - \hat{\theta}\right)^2
\]

\hypertarget{comparison-of-jackknife-and-bootstrap-resampling}{%
\section{Comparison of Jackknife and Bootstrap
Resampling}\label{comparison-of-jackknife-and-bootstrap-resampling}}

Given their comparable construction and application, the jackknife and
bootstrap make similar assumptions regarding the data. Most notably,
both assume that the function used to estimate the parameter is smooth.
Formally, a smooth function is defined as one that has continuous
derivatives over some domain, with the minimum number of derivatives
required to be considered smooth varying per the question at hand
(Weisstein, n.d.). From a statistical perspective, if the function used
to estimate the parameter is smooth on some domain \((a, b)\),
\([F(q_i) - F(q_z)] \rightarrow 0\) as
\(\lvert q_i - q_z\rvert \rightarrow 0\). Meaning, among a set of
conceivable, non-identical samples from the population, minor
differences between possible samples will only result in minor
differences between the statistic estimated (Chen and Shao 2001). Due to
its deterministic nature, the jackknife tends to perform poorly in
estimating non-smooth statistics (Wicklin 2017). However, its
deterministic nature also yields the jackknife superior to bootstrapping
in smaller datasets. To combat the issue regarding non-smooth
statistics, a generalized jackknife resampling scheme, called the
\emph{drop-d-jackknife} has been proposed, which was utilized in this
estimator (Chen and Shao 2001).

\hypertarget{multiple-imputation}{%
\section{Multiple Imputation}\label{multiple-imputation}}

Multiple imputation is a missing data management method based on both
Bayesian and frequentist inference proposed by Donald Rubin in the late
1970s (Jonathan W. Bartlett and Hughes 2020; Buuren 2012). Before
Rubin's proposal, statisticians had been utilizing various
single-imputation methods and proceeding with the analyses of interest
as if data had not been missing. However, Rubin noted that
single-imputation methods could not accurately capture the uncertainty
caused by the missing observations (Buuren 2012). In response, he
proposed imputing any given datum with a series of plausible values from
its posterior distribution, thus creating several complete versions of
the observed dataset and applying the complete data analysis procedure
to each generated dataset. He proposed a series of rules that could
derive point and variance estimates that would adequately reflect the
uncertainty caused by the missing observations.

Rubin's idea to utilize multiple imputation was ridiculed at first, not
only due to its drastically different interpretation of uncertainty but
because of how unfeasible it was at the time (Buuren 2012). Rubin's
method would require statisticians to come up with an imputation model
that would allow them to draw values from the posterior distribution.
After that, they would have to draw several values and repeat the
analysis multiple times with the numerous complete datasets. The
preceding workflow was challenging to implement in an era of low
computational power and expensive digital storage (Buuren 2012). As
such, Rubin's ideas did not receive immediate acceptance. However, since
the late 1990s, with the increased access to computers and user-friendly
statistical packages capable of implementing complex procedures,
multiple imputations have been adopted and heavily researched, leading
to various modified algorithms being utilized under different conditions
to obtain valid inferences in the presence of missing data (Buuren
2012). At this time, one of the most notable challenges remaining with
multiple imputation is the concept of congeniality. Congeniality may be
thought of as the imputation model and the analysis model making
compatible assumptions regarding the data. In the early 1990s, Fay and
Meng demonstrated that congeniality was required to obtain valid
inferences from multiple imputation (Jonathan W. Bartlett and Hughes
2020; Jonathan W. Bartlett 2021).

\hypertarget{current-state-of-multiple-imputation}{%
\subsection{Current State of Multiple
Imputation}\label{current-state-of-multiple-imputation}}

Today, with the advent of public databases, the imputer and the analyst
may no longer be the same person. Even in the absence of such cases, the
imputation and analysis model may still be uncongenial if there does not
exist a unifying Bayesian model which embeds the imputer's imputation
model and the analyst's complete data procedure (Jonathan W. Bartlett
2021). As such, researchers have begun to develop approaches that
combine resampling methods with multiple imputation to obtain valid
inferences even under uncongeniality. Of the currently proposed methods,
there exist two main limitations: a) the increased computational cost
brought on by resampling methods alongside multiple imputation, and b)
inference in smaller sample sizes. At this time, nearly all mainstream
approaches proposed by researchers utilize bootstrap resampling and
multiple imputation to obtain valid inferences under uncongeniality;
however, as discussed above, bootstrap resampling requires a certain
sample size to provide proper estimates. As a viable alternative to be
utilized in instances where there is a small sample size, a jackknife
variance estimator is proposed.

\bookmarksetup{startatroot}

\hypertarget{methods}{%
\chapter{Methods}\label{methods}}

For the proposed Monte Carlo simulation, \(N\) = 10,000 datasets were
generated with the following characteristics: A response variable,
\(Y\), where 30\% of the observations were missing at random (MAR), and
an \(n \times q\) matrix of fully observed covariates, where \(n = 50\)
was the sample size, and \(q = 3\) the number of covariates.

Formally

\[
\begin{bmatrix} V1 \\V2 \\ V3 \end{bmatrix} \sim N\left(\begin{bmatrix} 1\\ 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 & 0.5 & 0.5 \\
0.5 & 1 & 0.5 \\
0.5 & 0.5 & 1
\end{bmatrix}\right)
\]

With

\[
\beta_{V_1} = 2 ; \  \beta_{V_2} = 5 ; \ \beta_{V_3} = 8
\]

The outcome variable was simulated such that:

\[
Y = V_1 + V_2 + V_3 + \epsilon
\]

Where

\[
\epsilon \sim N(\mu = 0, \sigma \propto V_2)
\]

Thus, data were simulated with heteroskedastic errors, which yields the
imputation and analysis models congenial, yet misspecified.

Formally, the analysis model of interest was

\[
\widehat{Y} \sim \widehat{\beta}_{V_1} + \widehat{\beta}_{V_2} + \widehat{\beta}_{V_3} 
\]

And the imputation model was

\[
\widehat{Y}_{\text{mis}} \sim \widehat{\beta}_{V_1} + \widehat{\beta}_{V_2} + \widehat{\beta}_{V_3}
\]

Where the imputation method of choice was predictive mean matching
(PMM).

All generated datasets were analysed using three approaches:

\begin{itemize}
\item
  \textbf{Bootstrap then multiply impute:} The observed dataset with
  missing observations was initially bootstrapped \(n = 200\) times.
  Thereafter, each of the bootstrap samples were imputed \(m = 2\)
  times, with a maximum of \(maxit = 5\) iterations. The mean of the
  bootstrap estimates served as the final point estimate, and a 95\%
  confidence interval was generated through the percentile method, where
  the \(\alpha/2^{th}\) and \(1-(\alpha/2)^{th}\) percentiles were the
  lower and upper bounds, respectively. The R package, \emph{bootImpute}
  was be utilized for this process.
\item
  \textbf{Multiply impute then use Rubin's rules:} The observed dataset
  with missing observations was imputed \(m = 10\) times, with a maximum
  of \(maxit = 5\) iterations. The point estimate, as well as the
  confidence interval was obtained through the following rules proposed
  by Donald Rubin. The \emph{mice} package was utilized for this
  process.
\end{itemize}

\begin{align}
\bar{\theta} &= \frac{1}{m}\left (\sum_{i=1}^m{\theta_i}\right ) \\ 
V_{\text{within}} &= \frac{1}{m} \sum_{i=1}^m{SE_i^2} \\ 
V_{\text{between}} &= \frac{\sum_{i=1}^m (\theta_i - \overline{\theta})^2}{m-1} \\ 
V_{\text{total}} &= V_W + V_B + \frac{V_B}{m} \\ 
\bar{\theta} &\pm t_{df,1-\alpha/2} * \sqrt{V_{\text{total}}} 
\end{align}

\begin{itemize}
\tightlist
\item
  \textbf{Jackknife then multiply impute:} Although a more detailed
  overview of the proposed jackknife estimator is provided in
  Chapter~\ref{sec-est}, briefly, the observed dataset was jackknifed to
  obtain \(j = 200\) samples, each of which were imputed \(m = 2\)
  times, with a maximum of \(maxit = 5\) iterations. The mean of the
  jackknife estimates served as the final point estimate, and a 95\%
  confidence interval was generated through the percentile method, where
  the \(\alpha/2^{th}\) and \(1-(\alpha/2)^{th}\) percentiles were the
  lower and upper bounds, respectively.
\end{itemize}

Thereafter, the methods examined were compared concerning their point
estimates, confidence intervals, and computational expense.

All analyses were be conducted using R 4.2.1 (Funny-Looking Kid).

\bookmarksetup{startatroot}

\hypertarget{sec-est}{%
\chapter{The Proposed Jackknife Estimator}\label{sec-est}}

A pseudocode overview of the jackknife estimator proposed may be seen in
the following figure.

\begin{figure}

{\centering \includegraphics{./Algorithm_for_jackknife_estimator.jpg}

}

\caption{A pseudocode depiction of the proposed estimator.}

\end{figure}

Briefly, the algorithm begins by obtaining \(j\) jackknife subsamples
from the observed dataset with missing observations. Thereafter, each of
the \(j\) subsamples are imputed \(m\) times, resulting in a total of
\(j \times m\) complete datasets. Subsequently, the analysis model of
interest to estimate \(\theta\) is applied to each of the completed
datasets to produce
\(\hat{\theta^*}_1, \hat{\theta^*}_2, \hat{\theta^*}_3, \dots, \hat{\theta^*}_{n \choose d}\).
The point estimate then becomes the mean of the previously produced
\(n\) pseudo-estimates, with the confidence interval the
\(\alpha/2^{th}\) and \(1-\alpha/2^{th}\), respectively.

As part of the algorithm, researchers must choose values \(d\) and
\(j\), which will be context-dependent quantities. Ideally, a \(d\)
value which satisfies \(\frac{\sqrt{n}}{d} \rightarrow 0\) will provide
asymptotically unbiased estimates even for non-smooth statistics (Chen
and Shao 2001; Shao and Wu 1989). Rewriting the foregoing condition for
\(d\)

\begin{align}
&\frac{\sqrt{n}}{d} \rightarrow \ 0 \\ 
&\implies d >> \sqrt{n} \\ 
&\text{Since} \ d < n \\
&\implies n > d >> \sqrt{n}
\end{align}

It is evident that \(d\) should take on some value between \(n\) and
\(\sqrt{n}\) with \(d\) being closer to \(n\), particularly for
non-smooth statistics. At any rate, \(j = \binom{n}{d}\) will likely be
a value that is not computationally feasible to obtain. As such, the
number of subsamples required, \(j\), can be limited to yield the
estimator more accessible. The choice of \(j\) will be a multifaceted
decision, where, if possible, greater values are preferred. Ideally, a
small pilot study may be performed with a range of \(j\) values to
determine values of \(j\) for which estimates begin to converge.

Although not as widely applicable, researchers may consider utilizing a
delete-one jackknife, as discussed in Section~\ref{sec-del} Given the
stochastic nature of multiple imputation, especially in instances where
a high proportion of missingness is present, the pseudo-estimates may
vary widely between any two given jackknife subsamples, similar to what
would be observed in the case of percentiles. As such, the delete-one
jackknife approach is not recommended for general use but could be
considered in samples with low missingness proportion.

\bookmarksetup{startatroot}

\hypertarget{results}{%
\chapter{Results}\label{results}}

Per the Methods section, the performance of the proposed jackknife
estimator was compared to two leading methods in the literature, Rubin's
rules following multiple imputation and Bootstrap resampling prior to
multiple imputation. The methods were compared concerning the coverage
probabilities they generated, the widths of their respective confidence
intervals, their computational expense, and the bias of their point
estimators.

\hypertarget{point-estimates}{%
\section{Point Estimates}\label{point-estimates}}

All methods, perhaps with the exception of Rubin's rules, produced
reasonable point estimates with minimal bias. Rubin's rules resulted in
slightly anticonservative point estimates with greater standard
deviation, indicative of a statistically inefficient estimator with high
variability. This finding is unsurprising given the literature on
Rubin's rules and its performance under uncongeniality.

In contrast, it is noted that both resampling methods examined provide
nearly unbiased point estimates with smaller standard deviations
compared to Rubin's rules. Again, given the literature on
uncongeniality, this finding was unsurprising; however, given the lack
of both empirical and theoretical justification for the bootstrap
approach in small sample sizes, the desirable properties noted are
worthy of further examination. Between all three methods, nevertheless,
it is noted that the proposed jackknife estimator produced estimates
with the smallest amount of bias, as well as the smallest standard
deviation, an observation perhaps better observed in
Section~\ref{sec-dist}, where the distribution of the biases of the
point estimates is compared. From the kernel density plots presented, it
is evident that Rubin's rules slightly underestimate the parameter.
Compared to Rubin's rules, both resampling approaches provide point
estimates that are nearly unbiased; however, it is noted that the
bootstrap estimates, despite being centered near zero, are slightly more
dispersed compared to the jackknife estimates, which present as a narrow
distribution centered at zero. This observation is justified by the
values provided in Section~\ref{sec-point} which demonstrate that the
jackknife point estimates have a smaller standard deviation than the
bootstrap approach and Rubin's rules.

From a point estimation perspective, the statistically desirable
properties of the jackknife estimator, alongside its theoretical and
empirical justification when used with small sample sizes, yield it a
desirable estimator.

\hypertarget{sec-point}{%
\subsection{Summary of Point Estimation Properties}\label{sec-point}}

\begin{tabular}{l|r|r|r}
\hline
Method & Median Point Estimates  & Mean Point Estimates & SD of Point Estimates\\
\hline
Jackknife & 1.971975 & 1.965713 & 0.3648634\\
\hline
Bootstrap & 1.928350 & 1.921044 & 0.4442088\\
\hline
Rubin's Rules & 1.788012 & 1.750637 & 0.4962010\\
\hline
\end{tabular}

\hypertarget{sec-dist}{%
\subsection{Distribution of Point Estimator Bias}\label{sec-dist}}

\includegraphics{./results_files/figure-pdf/unnamed-chunk-3-1.pdf}

\hypertarget{sec-ci}{%
\section{Confidence Intervals}\label{sec-ci}}

Over-coverage of confidence intervals is noted across all methods;
however, particulary with the jackknife and bootstrap approaches, such
over-coverage is only slightly over the nominal, as such, they are
likely not of concern and can be explained, in part, by the Monte Carlo
standard error for the true coverage proabability of a 95\% confidence
interval. Among the three methods noted, Rubin's rules, by a significant
margin, deviates from the nominal coverage, indicative of an overly
conservative estimator. An argument could be made, particulary in the
case of biological studies that an overly conservative estimator is
safer than one that is anti-conservative; however, the statistical
inefficieny created by conservative estimators can be of concern,
particulary in instances where small sample sizes are present or the
test being utilized already has low statistical power. Comparing these
methods concerning confidence interval width, it is noted that both
resampling approaches provide narrower confidence intervals, with the
jackknife approach providing the narrowest confidence intervals by a
wide margin. In instances where nominal, or near-nominal coverage is
reached, narrower confidence intervals are indicative of more efficient
estimators. Given the near-nominal coverage noted with the jackknife
estimator, combined with the narrow confidence intervals it produces,
its superiority to the two other methods may be inferred.

A visual overview of the coverage probabilities may be noted in
Section~\ref{sec-zip}, where zipper plots of the methods are presented
utilizing a simple random sample of 100 observations from the Monte
Carlo simulation results of all methods examined. Given the small number
of subsamples examined for visual clarity, the plots may not be
indicative of the larger results presented above; however, they provide
an appreciation for the meaning of coverage probabilities.

\begin{tabular}{l|r|r|r|r}
\hline
Method & Coverage Probability & Median C.I. Width & Mean C.I. Width & SD of C.I. Width\\
\hline
Jackknife & 97.67 & 1.610436 & 1.683248 & 0.5197257\\
\hline
Bootstrap & 97.69 & 2.161853 & 2.228137 & 0.6423704\\
\hline
Rubin's Rules & 98.46 & 2.455640 & 2.537168 & 0.7230490\\
\hline
\end{tabular}

\hypertarget{sec-zip}{%
\subsection{Zipper Plots for Confidence Interval
Coverage}\label{sec-zip}}

\includegraphics{./results_files/figure-pdf/unnamed-chunk-5-1.pdf}

\hypertarget{performance-benchmark-results}{%
\section{Performance Benchmark
Results}\label{performance-benchmark-results}}

Finally, the three methods are compared concerning their computational
expenses. Comparing the two approaches, which require further
resampling, it is noted that the bootstrap approach takes nearly ten
times longer than the jackknife approach per iteration. Unsurprisingly,
Rubin's rules, which do not rely on any further resampling besides the
one performed during multiple imputation, was the fastest approach.
However, given its biased estimates under uncongeniality or
misspecification, the computational advantage it brings to the table
adds very little value.

Despite generating the same number of subsamples \((n = 200)\), albeit
in contrasting manners, with the same number of imputations \((m = 2)\)
and iterations \((maxit = 5)\), it is surprising that the bootstrap
approach took nearly ten times longer per iterations to provide
estimates. Regardless, the significantly reduced computational expense
of the jackknife estimator yields it superior to the bootstrap approach
under this particular scenario.

\begin{tabular}{l|r|r|r}
\hline
Method & Median Time (seconds/iteration) & Mean Time (seconds/iteration) & SD of Time (seconds/iteration)\\
\hline
Jackknife & 8.658003 & 8.800480 & 0.6319727\\
\hline
Bootstrap & 72.480187 & 73.012739 & 1.6246321\\
\hline
Rubin's Rules & 3.188039 & 3.183781 & 0.0913400\\
\hline
\end{tabular}

\includegraphics{./results_files/figure-pdf/unnamed-chunk-7-1.pdf}

\bookmarksetup{startatroot}

\hypertarget{discussion}{%
\chapter{Discussion}\label{discussion}}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

In this paper, a jackknife estimator for multiply imputed outcome
variables under the concern of uncongeniality was presented. The
proposed estimator was compared to two alternative approaches in the
literature employing a Monte Carlo simulation study, where all methods
were evaluated on the bias of their point estimates, the width and
coverage of their confidence intervals, and computational time. All
procedures were found to slightly over-cover, suggesting conservative
variance estimates, with Rubin's rules resulting in the broadest
confidence intervals with significant over-coverage compared to the
nominal level. In contrast, the two resampling-based approaches examined
resulted in a substantial decline in confidence interval width, with the
proposed jackknife estimator providing the narrowest confidence
intervals by a wide margin while still attaining near-nominal coverage.

Unsurprisingly, Rubin's rules were the least computationally costly
approach among the ones examined; however, given its downward-biased
point estimates and wide confidence intervals, particularly in instances
where uncongeniality is a concern, resampling-based robust methods
should be preferred. Among the two resampling-based methods examined,
the bootstrap approach took nearly ten times longer per iteration, which
was a surprising observation, as both the jackknife and bootstrap
methods utilized the same number of imputations, iterations, and
subsamples. From a computational perspective, there is no evident reason
why the bootstrap method should take nearly ten times longer than the
proposed jackknife estimator. It is possible that the R package utilized
for the bootstrap approach, \emph{bootImpute}, is not optimized with
speed concerns in mind. However, neither was the jackknife estimator. In
both instances, the most time-consuming aspect of the processes was the
imputation of the subsamples generated, either via bootstrap resampling
or jackknife resampling, done with the same R package, \emph{mice}, and
the same parameters, number of iterations, and imputations. Given that
the foregoing steps took place under identical conditions, perhaps a
different aspect of the two approaches could explain the discrepancy
noted.

Nevertheless, given the superior performance of the jackknife estimator
noted, alongside its computational efficiency, the recommendation to
replace Rubin's rules as the de facto standard in small, multiply
imputed datasets with our proposed estimator is made.

\hypertarget{future-directions}{%
\section{Future Directions}\label{future-directions}}

Perhaps the most significant issue noted with the jackknife estimator
was a slightly higher coverage probability compared to the nominal.
Although the Monte Carlo error may partially explain this observation,
alternative confidence interval construction approaches could be
considered to yield more appropriate coverage. Namely, it is possible
that a confidence interval could be generated by examining various
values calculated based on Rubin's rules, such as the fraction of
missing information, relative efficiency, and relative increase in
variance. The aforestated statistics could be used to appropriately
model the uncertainty in the imputation process, which, in turn, could
be used to generate confidence intervals in a semi-parametric manner.

Otherwise, although the proposed estimator was efficient even with a
small number of subsamples and imputations, alternative confidence
interval constructions could allow one to utilize even fewer subsamples
while still attaining nominal coverage, which may make the proposed
method even more computationally feasible.

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Anderson_Gerbing_1984}{}}%
Anderson, James C., and David W. Gerbing. 1984. {``The Effect of
Sampling Error on Convergence, Improper Solutions, and Goodness-of-Fit
Indices for Maximum Likelihood Confirmatory Factor Analysis.''}
\emph{Psychometrika} 49 (2): 155--73.
\url{https://doi.org/10.1007/BF02294170}.

\leavevmode\vadjust pre{\hypertarget{ref-bartlett_reference-based_2021}{}}%
Bartlett, Jonathan W. 2021. {``Reference-Based Multiple
Imputation---What Is the Right Variance and How to Estimate It.''}
\emph{Statistics in Biopharmaceutical Research}, November, 1--9.
\url{https://doi.org/10.1080/19466315.2021.1983455}.

\leavevmode\vadjust pre{\hypertarget{ref-bartlett_bootstrap_2020}{}}%
Bartlett, Jonathan W, and Rachael A Hughes. 2020. {``Bootstrap Inference
for Multiple Imputation Under Uncongeniality and Misspecification.''}
\emph{Statistical Methods in Medical Research} 29 (12): 3533--46.
\url{https://doi.org/10.1177/0962280220932189}.

\leavevmode\vadjust pre{\hypertarget{ref-Bentler_Chou_1987}{}}%
Bentler, P. M., and Chih-Ping Chou. 1987. {``Practical Issues in
Structural Modeling.''} \emph{Sociological Methods \& Research} 16 (1):
78--117. \url{https://doi.org/10.1177/0049124187016001004}.

\leavevmode\vadjust pre{\hypertarget{ref-buuren_flexible_2012}{}}%
Buuren, Stef van. 2012. \emph{Flexible Imputation of Missing Data}.
Chapman \& Hall/{CRC} Interdisciplinary Statistics Series. Boca Raton,
{FL}: {CRC} Press.

\leavevmode\vadjust pre{\hypertarget{ref-chen_jackknife_2001}{}}%
Chen, Jiahua, and Jun Shao. 2001. {``Jackknife Variance Estimation for
Nearest-Neighbor Imputation.''} \emph{Journal of the American
Statistical Association} 96 (453): 260--69.
\url{https://doi.org/10.1198/016214501750332839}.

\leavevmode\vadjust pre{\hypertarget{ref-Faber_Fonseca_2014}{}}%
Faber, Jorge, and Lilian Martins Fonseca. 2014. {``How Sample Size
Influences Research Outcomes.''} \emph{Dental Press Journal of
Orthodontics} 19 (4): 27--29.
\url{https://doi.org/10.1590/2176-9451.19.4.027-029.ebo}.

\leavevmode\vadjust pre{\hypertarget{ref-fay1992inferences}{}}%
Fay, Robert E. 1992. {``When Are Inferences from Multiple Imputation
Valid?.''}

\leavevmode\vadjust pre{\hypertarget{ref-Jackson_2001}{}}%
Jackson, Dennis L. 2001. {``Sample Size and Number of Parameter
Estimates in Maximum Likelihood Confirmatory Factor Analysis: A Monte
Carlo Investigation.''} \emph{Structural Equation Modeling: A
Multidisciplinary Journal} 8 (2): 205--23.
\url{https://doi.org/10.1207/S15328007SEM0802_3}.

\leavevmode\vadjust pre{\hypertarget{ref-LaFontaine_2021}{}}%
LaFontaine, Denise. 2021. {``The History of Bootstrapping: Tracing the
Development of Resampling with Replacement.''} \emph{The Mathematics
Enthusiast} 18 (1--2): 78--99.
\url{https://doi.org/10.54870/1551-3440.1515}.

\leavevmode\vadjust pre{\hypertarget{ref-Mammen_1992}{}}%
Mammen, Enno. 1992. \emph{When Does Bootstrap Work?} Vol. 77. Lecture
Notes in Statistics. New York, NY: Springer New York.
\url{https://doi.org/10.1007/978-1-4612-2950-6}.

\leavevmode\vadjust pre{\hypertarget{ref-meng_multiple-imputation_1994}{}}%
Meng, Xiao-Li. 1994. {``Multiple-Imputation Inferences with Uncongenial
Sources of Input.''} \emph{Statistical Science} 9 (4): 538--58.
\url{https://doi.org/10.1214/ss/1177010269}.

\leavevmode\vadjust pre{\hypertarget{ref-rao_jackknife_1992}{}}%
Rao, J. N. K., and J. Shao. 1992. {``Jackknife Variance Estimation with
Survey Data Under Hot Deck Imputation.''} \emph{Biometrika} 79 (4):
811--22. \url{https://doi.org/10.2307/2337236}.

\leavevmode\vadjust pre{\hypertarget{ref-rubin1978multiple}{}}%
Rubin, Donald B. 1978. {``Multiple Imputations in Sample Surveys-a
Phenomenological Bayesian Approach to Nonresponse.''} In
\emph{Proceedings of the Survey Research Methods Section of the American
Statistical Association}, 1:20--34. American Statistical Association
Alexandria, VA, USA.

\leavevmode\vadjust pre{\hypertarget{ref-Shao_Wu_1989}{}}%
Shao, Jun, and C. F. J. Wu. 1989. {``A General Theory for Jackknife
Variance Estimation.''} \emph{The Annals of Statistics} 17 (3):
1176--97. \url{https://doi.org/10.1214/aos/1176347263}.

\leavevmode\vadjust pre{\hypertarget{ref-Wang_1998}{}}%
Wang, N. 1998. {``Large-Sample Theory for Parametric Multiple Imputation
Procedures.''} \emph{Biometrika} 85 (4): 935--48.
\url{https://doi.org/10.1093/biomet/85.4.935}.

\leavevmode\vadjust pre{\hypertarget{ref-Weisstein}{}}%
Weisstein, Eric W. n.d. {``Smooth Function.''} Text.
\url{https://mathworld.wolfram.com/}.

\leavevmode\vadjust pre{\hypertarget{ref-wicklin_2017}{}}%
Wicklin, Rick. 2017. {``Jackknife Estimates in SAS.''} \emph{SAS Blog}.
\url{https://blogs.sas.com/content/iml/2017/06/21/jackknife-estimate-standard-error-sas.html}.

\leavevmode\vadjust pre{\hypertarget{ref-xie_dissecting_2016}{}}%
Xie, Xianchao, and Xiao-Li Meng. 2016. {``Dissecting Multiple Imputation
from a Multi-Phase Inference Perspective: What Happens When God's,
Imputer's and Analyst's Models Are Uncongenial?''} \emph{Statistica
Sinica}. \url{https://doi.org/10.5705/ss.2014.067}.

\end{CSLReferences}

\bookmarksetup{startatroot}

\hypertarget{appendix}{%
\chapter*{Appendix}\label{appendix}}
\addcontentsline{toc}{chapter}{Appendix}

\hypertarget{notation}{%
\section*{Notation}\label{notation}}
\addcontentsline{toc}{section}{Notation}

\begin{itemize}
\tightlist
\item
  \(N\) is the total number of units in the finite population being
  targeted.\\
\item
  \(X\) is an \(N \times q\) matrix of fully observed covariates.\\
\item
  \(Y\) is an \(N \times p\) matrix of partially observed outcome
  variables.\\
\item
  \(R\) is an \(N \times p\) matrix of response indicators (i.e.,
  \(R_{ij} = 1\) if the response on \(Y_{ij}\) is obtained and
  \(R_{ij} = 0\) otherwise.)\\
\item
  \(Q\) is an unknown quantity of interest to the analyst.
\item
  \(Z_c = \{X, Y_{inc}\}\) is the complete data.
\item
  \(Z_o = \{X, Y_{obs}, R_{inc}\}\) is the incomplete (i.e., observed)
  data.
\item
  The analyst's complete-data procedure is summarized by
  \(\mathscr{P}_{com} = [\hat{Q}(X, Y_{inc}), U(X, Y_{inc})]\), where
  \(\hat{Q}(X, Y_{inc})\) is an estimator of \(Q\) with associated
  variance \(U(X, Y_{inc}.)\)
\item
  \(R\) is not a part of \(\mathscr{P}_{com}\), as the missing at random
  assumption implies that the response behavior itself carries no
  information about \(Q\).
\end{itemize}

\hypertarget{formal-definition-of-congeniality}{%
\section*{Formal Definition of
Congeniality}\label{formal-definition-of-congeniality}}
\addcontentsline{toc}{section}{Formal Definition of Congeniality}

In short, one may define congeniality as the imputer and analyst making
different assumptions regarding the data. The following two-part formal
definition of uncongeniality was proposed by Meng in 1994, and will be
utilized in our research. Meeting the assumptions set forth in the
following two definitions qualifies the imputation model as being
congenial to the analysis model, or vice versa.

\hypertarget{definition-1}{%
\subsection*{Definition 1}\label{definition-1}}
\addcontentsline{toc}{subsection}{Definition 1}

Let \(E_f\) and \(V_f\) denote posterior mean and variance with respect
to \(f\), respectively. A Bayesian model \(f\) is said to be congenial
to the analysis procedure
\(\mathscr{P} \equiv \{\mathscr{P}_{obs}; \mathscr{P}_{com}\}\) for
given \(Z_o\) if the following hold:

\begin{itemize}
\item
  The posterior mean and variance of \(\theta\) under \(f\) given the
  incomplete data are asymptotically the same as the estimate and
  variance from the analyst's incomplete-data procedure
  \(\mathscr{P}_{obs}\), that is, \begin{equation}
      [\hat{\theta}(Z_o), U(Z_o)] \simeq [E_f[\theta | Z_o], V_f[\theta | Z_o]]
  \end{equation}
\item
  The posterior mean and variance of \(\theta\) under \(f\) given the
  complete data are asymptotically the same as the estimate and variance
  from the analyst's complete-data procedure \(\mathscr{P}_{com}\), that
  is, \begin{equation}
       [\hat{\theta}(Z_c), U(Z_c)] \simeq [E_f[\theta | Z_c], V_f[\theta | Z_c]]
  \end{equation}

  for any possible \(Y_{inc} = (Y_{obs}, Y_{miss})\) with \(Y_{obs}\)
  conditioned upon.
\end{itemize}

If the foregoing conditions are met, \(f\) is said to be second-moment
congenial to \(\mathscr{P}\).

\hypertarget{definition-2}{%
\subsection*{Definition 2}\label{definition-2}}
\addcontentsline{toc}{subsection}{Definition 2}

The analysis procedure \(\mathscr{P}\) is said to be congenial to the
imputation model \(g(Y_{miss}|Z_o, A)\) where \(A\) represents possible
additional data the imputer has access to, if one can find an \(f\) such
that (\textbf{i}) \(f\) is congenial to \(\mathscr{P}\) and
(\textbf{ii}) the posterior predictive density for \(Y_{miss}\) derived
under \(f\) is identical to the imputation model
\(f(Y_{miss}|Z_o) = g(Y_{miss}|Z_o, A) \ \forall \ Y_{miss}\).

\hypertarget{raw-data}{%
\section*{Raw Data}\label{raw-data}}
\addcontentsline{toc}{section}{Raw Data}

\begin{verbatim}
# A tibble: 10,000 x 13
   true_var UB_boot LB_boot point_es~1 point~2 LB_rub UB_rub    UB    LB point~3
      <dbl>   <dbl>   <dbl>      <dbl>   <dbl>  <dbl>  <dbl> <dbl> <dbl>   <dbl>
 1        2    2.76   0.971       2.04    2.13  1.07    3.20  2.62 1.54     2.19
 2        2    2.92   1.27        2.14    2.08  1.08    3.07  2.80 1.71     2.25
 3        2    2.90   0.937       1.94    2.10  1.09    3.11  2.76 1.30     2.03
 4        2    2.31   0.978       1.67    1.46  0.767   2.15  2.15 1.34     1.74
 5        2    3.13   1.08        2.03    2.12  1.30    2.94  2.68 1.01     1.87
 6        2    3.06   1.15        2.15    1.84  0.646   3.04  2.87 1.61     2.11
 7        2    2.68   1.23        1.95    1.80  0.728   2.87  2.60 1.46     1.98
 8        2    2.70  -0.184       1.52    1.33  0.132   2.53  2.52 0.418    1.54
 9        2    4.03   1.58        2.76    2.19  0.699   3.69  3.81 1.37     2.60
10        2    3.04   0.521       1.82    1.27 -0.252   2.78  2.90 0.998    1.89
# ... with 9,990 more rows, 3 more variables: rubin_width <dbl>,
#   jackknife_width <dbl>, boot_width <dbl>, and abbreviated variable names
#   1: point_estimate_boot, 2: point_estimate_rub, 3: point_estimate
\end{verbatim}

\hypertarget{code}{%
\section*{Code}\label{code}}
\addcontentsline{toc}{section}{Code}

Please see our publicly available GitHub
\href{https://github.com/ieb2/jackknife_var_est.git}{repo} for the
simulation code. Otherwise, the code utilized through the paper has been
included below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Aggregated code here for organization, make it available in an appendix.}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(ggpubr)}

\NormalTok{combined\_results }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"final\_sim\_data\_agg.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\StringTok{"...1"}\NormalTok{)}

\NormalTok{benchmark\_res }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"benchmark\_detailed\_res.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{X) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(expr) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\StringTok{"Mean Time (seconds/iteration)"} \OtherTok{=} \FunctionTok{mean}\NormalTok{(time)}\SpecialCharTok{/}\FloatTok{1e6}\SpecialCharTok{/}\DecValTok{100}\NormalTok{, }
            \StringTok{"Median Time (seconds/iteration)"} \OtherTok{=} \FunctionTok{median}\NormalTok{(time)}\SpecialCharTok{/}\FloatTok{1e6}\SpecialCharTok{/}\DecValTok{100}\NormalTok{, }
            \StringTok{"SD of Time (seconds/iteration)"} \OtherTok{=} \FunctionTok{sd}\NormalTok{(time)}\SpecialCharTok{/}\FloatTok{1e6}\SpecialCharTok{/}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\StringTok{"Method"} \OtherTok{=}\NormalTok{ expr)}

\NormalTok{benchmark\_res\_ord }\OtherTok{\textless{}{-}}\NormalTok{ benchmark\_res[}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),] }

\NormalTok{jackknife\_coverage }\OtherTok{\textless{}{-}} 
  \FunctionTok{round}\NormalTok{((}\FunctionTok{sum}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{true\_var }\SpecialCharTok{\textless{}}\NormalTok{ combined\_results}\SpecialCharTok{$}\NormalTok{UB }\SpecialCharTok{\&}
\NormalTok{               combined\_results}\SpecialCharTok{$}\NormalTok{true\_var }\SpecialCharTok{\textgreater{}}\NormalTok{ combined\_results}\SpecialCharTok{$}\NormalTok{LB) }\SpecialCharTok{/} \FunctionTok{nrow}\NormalTok{(combined\_results))}\SpecialCharTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{)}

\NormalTok{rubin\_coverage }\OtherTok{\textless{}{-}} 
  \FunctionTok{round}\NormalTok{((}\FunctionTok{sum}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{true\_var }\SpecialCharTok{\textless{}}\NormalTok{ combined\_results}\SpecialCharTok{$}\NormalTok{UB\_rub }\SpecialCharTok{\&}
\NormalTok{               combined\_results}\SpecialCharTok{$}\NormalTok{true\_var }\SpecialCharTok{\textgreater{}}\NormalTok{ combined\_results}\SpecialCharTok{$}\NormalTok{LB\_rub) }\SpecialCharTok{/} \FunctionTok{nrow}\NormalTok{(combined\_results))}\SpecialCharTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{)}

\NormalTok{boot\_coverage }\OtherTok{\textless{}{-}} 
  \FunctionTok{round}\NormalTok{((}\FunctionTok{sum}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{true\_var }\SpecialCharTok{\textless{}}\NormalTok{ combined\_results}\SpecialCharTok{$}\NormalTok{UB\_boot }\SpecialCharTok{\&}
\NormalTok{               combined\_results}\SpecialCharTok{$}\NormalTok{true\_var }\SpecialCharTok{\textgreater{}}\NormalTok{ combined\_results}\SpecialCharTok{$}\NormalTok{LB\_boot) }\SpecialCharTok{/} \FunctionTok{nrow}\NormalTok{(combined\_results))}\SpecialCharTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{)}

\NormalTok{ci\_sum }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \StringTok{"Method"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Jackknife"}\NormalTok{, }\StringTok{"Bootstrap"}\NormalTok{, }\StringTok{"Rubin\textquotesingle{}s Rules"}\NormalTok{), }
  
  \StringTok{"Coverage Probability"} \OtherTok{=} \FunctionTok{c}\NormalTok{(jackknife\_coverage, rubin\_coverage, boot\_coverage), }
  
  \StringTok{"Median C.I. Width"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{median}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{jackknife\_width), }\FunctionTok{median}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{boot\_width), }\FunctionTok{median}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{rubin\_width)), }
  
  \StringTok{"Mean C.I. Width"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{mean}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{jackknife\_width), }\FunctionTok{mean}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{boot\_width), }\FunctionTok{mean}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{rubin\_width)), }
  
  \StringTok{"SD of C.I. Width"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{sd}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{jackknife\_width), }\FunctionTok{sd}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{boot\_width), }\FunctionTok{sd}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{rubin\_width))}
\NormalTok{)}

\NormalTok{point\_estimate\_sum }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \StringTok{"Method"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Jackknife"}\NormalTok{, }\StringTok{"Bootstrap"}\NormalTok{, }\StringTok{"Rubin\textquotesingle{}s Rules"}\NormalTok{), }
  
  \StringTok{"Median Point Estimates "} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{median}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{point\_estimate), }\FunctionTok{median}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{point\_estimate\_boot), }\FunctionTok{median}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{point\_estimate\_rub)), }
  
  \StringTok{"Mean Point Estimates"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{mean}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{point\_estimate), }\FunctionTok{mean}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{point\_estimate\_boot), }\FunctionTok{mean}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{point\_estimate\_rub)), }
  
  \StringTok{"SD of Point Estimates"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{sd}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{point\_estimate), }\FunctionTok{sd}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{point\_estimate\_boot), }\FunctionTok{sd}\NormalTok{(combined\_results}\SpecialCharTok{$}\NormalTok{point\_estimate\_rub))}
\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{234}\NormalTok{)}
\NormalTok{jackk }\OtherTok{\textless{}{-}}\NormalTok{ combined\_results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{sample\_n}\NormalTok{(}\FloatTok{1e2}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{covers =} \FunctionTok{ifelse}\NormalTok{(UB }\SpecialCharTok{\textgreater{}}\NormalTok{ true\_var }\SpecialCharTok{\&}\NormalTok{ true\_var }\SpecialCharTok{\textgreater{}}\NormalTok{ LB, }\StringTok{"Covers"}\NormalTok{, }\StringTok{"Does Not Cover"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(., }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ LB, }\AttributeTok{ymax =}\NormalTok{ UB, }\AttributeTok{color =}\NormalTok{ covers)) }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \CommentTok{\#coord\_flip() + }
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ latex2exp}\SpecialCharTok{::}\FunctionTok{TeX}\NormalTok{(}\StringTok{"$i\^{}\{th\} iteration"}\NormalTok{), }
    \AttributeTok{y =} \StringTok{"C.I."}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =}\NormalTok{ combined\_results}\SpecialCharTok{$}\NormalTok{true\_var) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{name =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{"\#999999"}\NormalTok{, }\StringTok{"\#FF0000"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Jackknife Estimator"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlim}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{ylim}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{2.5}\NormalTok{,}\DecValTok{5}\NormalTok{))}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{24123}\NormalTok{)}
\NormalTok{boot }\OtherTok{\textless{}{-}}\NormalTok{ combined\_results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{sample\_n}\NormalTok{(}\FloatTok{1e2}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{covers =} \FunctionTok{ifelse}\NormalTok{(UB\_boot }\SpecialCharTok{\textgreater{}}\NormalTok{ true\_var }\SpecialCharTok{\&}\NormalTok{ true\_var }\SpecialCharTok{\textgreater{}}\NormalTok{ LB\_boot, }\StringTok{"Covers"}\NormalTok{, }\StringTok{"Does Not Cover"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(., }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ LB\_boot, }\AttributeTok{ymax =}\NormalTok{ UB\_boot, }\AttributeTok{color =}\NormalTok{ covers)) }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \CommentTok{\#coord\_flip() + }
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ latex2exp}\SpecialCharTok{::}\FunctionTok{TeX}\NormalTok{(}\StringTok{"$i\^{}\{th\} iteration"}\NormalTok{), }
    \AttributeTok{y =} \StringTok{"C.I."}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =}\NormalTok{ combined\_results}\SpecialCharTok{$}\NormalTok{true\_var) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{name =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{"\#999999"}\NormalTok{, }\StringTok{"\#FF0000"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Bootstrap Estimator"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlim}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{ylim}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{2.5}\NormalTok{,}\DecValTok{5}\NormalTok{))}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{234}\NormalTok{)}
\NormalTok{rubin }\OtherTok{\textless{}{-}}\NormalTok{ combined\_results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{sample\_n}\NormalTok{(}\FloatTok{1e2}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{covers =} \FunctionTok{ifelse}\NormalTok{(UB\_rub }\SpecialCharTok{\textgreater{}}\NormalTok{ true\_var }\SpecialCharTok{\&}\NormalTok{ true\_var }\SpecialCharTok{\textgreater{}}\NormalTok{ LB\_rub, }\StringTok{"Covers"}\NormalTok{, }\StringTok{"Does Not Cover"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(., }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ LB\_rub, }\AttributeTok{ymax =}\NormalTok{ UB\_rub, }\AttributeTok{color =}\NormalTok{ covers)) }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \CommentTok{\#coord\_flip() + }
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ latex2exp}\SpecialCharTok{::}\FunctionTok{TeX}\NormalTok{(}\StringTok{"$i\^{}\{th\} iteration"}\NormalTok{), }
    \AttributeTok{y =} \StringTok{"C.I."}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =}\NormalTok{ combined\_results}\SpecialCharTok{$}\NormalTok{true\_var) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{name =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{"\#999999"}\NormalTok{, }\StringTok{"\#FF0000"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Rubin\textquotesingle{}s Rules"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlim}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{ylim}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{2.5}\NormalTok{,}\DecValTok{5}\NormalTok{))}

\NormalTok{point\_dist }\OtherTok{\textless{}{-}}\NormalTok{ reshape2}\SpecialCharTok{::}\FunctionTok{melt}\NormalTok{(combined\_results[}\FunctionTok{c}\NormalTok{(}\StringTok{"point\_estimate\_rub"}\NormalTok{, }\StringTok{"point\_estimate"}\NormalTok{, }\StringTok{"point\_estimate\_boot"}\NormalTok{)]) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ .,}
         \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ value }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ variable, }\AttributeTok{color =}\NormalTok{ variable)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ ..density..), }\AttributeTok{alpha =} \FloatTok{0.25}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.title.y =} \FunctionTok{element\_blank}\NormalTok{(), }
        \AttributeTok{panel.spacing=}\FunctionTok{unit}\NormalTok{(}\FloatTok{1.5}\NormalTok{,}\StringTok{"lines"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.title.y =} \FunctionTok{element\_blank}\NormalTok{(), }
        \AttributeTok{panel.spacing=}\FunctionTok{unit}\NormalTok{(}\FloatTok{1.5}\NormalTok{,}\StringTok{"lines"}\NormalTok{), }
        \AttributeTok{strip.text =} \FunctionTok{element\_text}\NormalTok{(}
          \AttributeTok{size =} \DecValTok{9}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ latex2exp}\SpecialCharTok{::}\FunctionTok{TeX}\NormalTok{(}\StringTok{"$}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{widehat\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{beta\_1\} {-} }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{beta\_1"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{scale\_fill\_discrete}\NormalTok{(}\AttributeTok{name =} \StringTok{"Method"}\NormalTok{, }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Rubin\textquotesingle{}s Rules"}\NormalTok{, }\StringTok{"Jackknife"}\NormalTok{, }\StringTok{"Bootstrap"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_discrete}\NormalTok{(}\AttributeTok{name =} \StringTok{"Method"}\NormalTok{, }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Rubin\textquotesingle{}s Rules"}\NormalTok{, }\StringTok{"Jackknife"}\NormalTok{, }\StringTok{"Bootstrap"}\NormalTok{))}

\NormalTok{benchmark\_plot }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"benchmark\_detailed\_res.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{X) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(., }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ expr, }\AttributeTok{y =}\NormalTok{ time}\SpecialCharTok{/}\FloatTok{1e6}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_violin}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_log10}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Benchmark for Methods Examined"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{y =} \StringTok{"Time (seconds/iteration)"}
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.title.y =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}




\end{document}
