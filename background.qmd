# Background

Missing data is a discipline-agnostic issue commonly encountered by statistical practitioners. Given that many statistical procedures require data to be complete (i.e., in the form of an $n \times m$ matrix), the appropriate course of action to be taken in the presence of missing data has long been investigated by statisticians. Today, multiple imputation is accepted as the gold standard in missing data analysis, thanks to the work of Donald Rubin [@buuren_flexible_2012]. In 1977, Rubin proposed using multiple completed versions of the dataset with missing observations, applying the complete-data procedure of interest, and pooling the estimates to draw valid inferences [@buuren_flexible_2012]. The main advantage of multiple imputation, as opposed to single imputation, which had been used by researchers since the early 1970s, is its ability to properly estimate the variance caused by missing observations [@buuren_flexible_2012]. The emphasis placed on variance and uncertainty by Rubin was a departure from the status quo of the time, which was to fill in the missing observation with the most probable value and to proceed with complete case analyses as if the observation had not been missing, to begin with [@buuren_flexible_2012]. This approach, however, fails to incorporate the loss of information caused by missing observations into the estimation of parameters, resulting in the underestimation of variance [@rubin1978multiple].

Like all revolutionary ideas, multiple imputation received harsh criticism following its conception. Perhaps the most notable of the objections came from Fay in 1992, who demonstrated through counterexamples that multiple imputation produced biased covariance estimates [@bartlett_bootstrap_2020]. Fay added that the need for unison between the imputation and analysis model made multiple imputation a poor general-purpose tool, particularly in instances where the imputer and analyst are different individuals [@fay1992inferences; @buuren_flexible_2012]. Fay's arguments led to the conceptualization of congeniality between the imputation and analysis model, which was later accepted to be a requirement to obtain valid inferences from multiple imputation using Rubin's pooling rules (hereafter, Rubin's rules) [@buuren_flexible_2012; @meng_multiple-imputation_1994]. Although Fay's work initially criticized biases introduced to the covariance matrix following multiple imputation, a similar phenomenon of biased estimations were observed with variance under uncongeniality [@fay1992inferences; @meng_multiple-imputation_1994; @xie_dissecting_2016].

Some of the earliest works demonstrating Rubin's variance estimator to be biased under uncongeniality were from Wang and Robins in 1998, who also proposed an alternate variance estimator in the same paper [@buuren_flexible_2012]. The variance estimator proposed by Wang and Robins requires the calculation of several quantities, which are not easily accessible to the average statistical practitioner [@bartlett_bootstrap_2020]. The challenging construction of the variance estimator proposed by Wang and Robins has resulted in it receiving little-to-no attention in applied settings [@bartlett_bootstrap_2020]. In an attempt to create a more user-friendly variance estimator in instances of suspected uncongeniality, researchers have proposed combining resampling methods with multiple imputation. Of the two main resampling methods, bootstrap has received more attention from multiple imputation researchers compared to jackknife resampling, which has mostly been investigated under single hot-deck imputation. Although particular combinations of bootstrap and multiple imputation have been demonstrated to create asymptotically unbiased estimates of variance, the associated computational cost makes this an active area of research [@bartlett_bootstrap_2020]. Most recently, von Hippel has proposed a bootstrap variance estimator which addresses the issue of computational cost; however, it has been demonstrated to create confidence intervals that are slightly wider compared to traditional bootstrap and multiple imputation combinations [@bartlett_bootstrap_2020]. Given the lower computational cost associated with jackknife resampling, as well as desirable properties demonstrated under single imputation, such as being unbiased in certain scenarios, it is an attractive alternative that should be considered as a variance estimator of multiply imputed data under uncongeniality [@chen_jackknife_2001; @rao_jackknife_1992]. More importantly, however, is the advantages jackknife resampling has over bootstrap resampling in small sample sizes, which are frequently encountered in datasets associated with biological studies.

## Bootstrap Resampling

Let $q$ be the set of observations $\left(z_1, z_2, z_3, \dots, z_n \right)$ from the population $Q$ such that $z_i \ \forall \ i \in \{1, 2, 3, \dots, n\}$ is an i.i.d sample from $Q$. Moreover, let $\theta$ be some parameter of interest, with the unbiased estimator $\hat{\theta}$, which is a statistic computed from $F(q)$. Finally, let $G_{\theta}$ be the sampling distribution of $F(q)$. The non-parametric bootstrap, as proposed by Efron, lets $q$ define $Q$ such that the set of observations $\left(z_1, z_2, z_3, \dots, z_n \right)$ appears with equal proportion in the infinitely large population. From there, the set of samples $q^*_1, q^*_2, q^*_3, \dots, q^*_m$ as $m \rightarrow \infty$ are generated by sampling $n$ observations with replacement from $q$, and the statistic of interest $\hat{\theta}$ is calculated by applying $F(q_i) \ \forall \ i \in \{1, 2, 3, \dots, m\}$, which results in $\hat{\theta^*}_1, \hat{\theta^*}_2, \hat{\theta^*}_3, \dots, \hat{\theta^*}_m$.

Finally, the point estimate is obtained by

$$\hat{\theta} = m^{-1}\sum^m_{i = 1} \hat{\theta^*}_i$$

and the variance is obtained by

$$\text{var}(\hat{\theta}) = m^{-1}\sum^m_{i = 1} \left(\hat{\theta^*}_i - \overline{\hat{\theta^*}_i}\right)^2$$

Since Efron's proposal of the non-parametric bootstrap, statisticians have widely utilized it thanks to its ease of implementation and the rapidly increasing computational power available to statistical practitioners. However, the properties of bootstrap resampling have their basis in the asymptotic theory, which holds in large sample sizes. The minimum sample size required to utilize bootstrap resampling and obtain asymptotically unbiased estimates is highly context-dependent; in certain situations, a minimum sample size of $n = 200$ has been suggested, with other authors suggesting between $n = 100$ to $n = 500$. Given that in many biological studies, the minimum sample size required for asymptotically unbiased estimates may not be achieved, jackknife resampling, a method that predates the bootstrap, may be considered.

## Jackknife Resampling

Let $q$ be the set of observations $\left(z_1, z_2, z_3, \dots, z_n \right)$ from the population $Q$ such that $z_i \ \forall \ i \in \{1, 2, 3, \dots, n\}$ is an i.i.d sample from $Q$. Moreover, let $\theta$ be some parameter of interest, with the unbiased estimator $\hat{\theta}$, which is a statistic computed from $F(q)$. Finally, let $G_{\theta}$ be the sampling distribution of $F(q)$. The jackknife, as proposed by Quenouille and expanded on by Tukey, creates $n$ leave-one-out subsamples from $q$ such that $q_{-1} = \left(z_2, z_3, z_4, \dots, z_n \right),$ $q_{-2} = \left(z_1, z_3, z_4, \dots, z_n \right), \dots, q_{-n} = \left(z_1, z_2, z_3, \dots, z_{n-1} \right)$ and $\lvert q_{-i}\rvert = n-1 \ \forall i \in \{1, 2, 3, \dots, n\}$. Thereafter, the statistic of interest $\hat{\theta}$ is calculated by applying $F(q_i) \ \forall \ i \in \{1, 2, 3, \dots, n\}$, which results in $\hat{\theta^*}_1, \hat{\theta^*}_2, \hat{\theta^*}_3, \dots, \hat{\theta^*}_m$.

Finally, the point estimate is obtained by

$$\hat{\theta} = n^{-1}\sum^n_{i = 1} \hat{\theta^*}_i$$

and the variance is obtained by

$$\text{var}(\hat{\theta}) = n^{-1}\sum^n_{i = 1} \left(\hat{\theta^*}_i - \overline{\hat{\theta^*}_i}\right)^2$$

## Comparison of Jackknife and Bootstrap Resampling

Given their comparable construction and application, the jackknife and bootstrap make similar assumptions regarding the data. Most notably, both assume that the function used to estimate the parameter is smooth. Formally, a smooth function is defined as one that has continuous derivatives over some domain, with the minimum number of derivatives required to be considered smooth varying per the question at hand. From a statistical perspective, if the function used to estimate the parameter is smooth on some domain $(a, b)$, $[F(q_i) - F(q_z)] \rightarrow 0$ as $\lvert q_i - q_z\rvert \rightarrow 0$. Meaning, among a set of conceivable, non-identical samples from the population, minor differences between possible samples will only result in minor differences between the statistic estimated. Due to its deterministic nature, the jackknife tends to perform poorly in estimating non-smooth statistics. However, its deterministic nature also yields the jackknife superior to bootstrapping in smaller datasets. 
