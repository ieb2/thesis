 @inproceedings{rubin1978multiple,
  title={Multiple imputations in sample surveys-a phenomenological Bayesian approach to nonresponse},
  author={Rubin, Donald B},
  booktitle={Proceedings of the survey research methods section of the American Statistical Association},
  volume={1},
  pages={20--34},
  year={1978},
  organization={American Statistical Association Alexandria, VA, USA}
}

@article{fay1992inferences,
  title={When Are Inferences from Multiple Imputation Valid?.},
  author={Fay, Robert E},
  year={1992},
  publisher={US Census Bureau [custodian]}
}

 @article{Wang_1998, title={Large-sample theory for parametric multiple imputation procedures}, volume={85}, ISSN={0006-3444, 1464-3510}, url={https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/85.4.935}, DOI={10.1093/biomet/85.4.935}, number={4}, journal={Biometrika}, author={Wang, N}, year={1998}, month={12}, pages={935–948}, language={en} }

 
 @article{Efron_1994, title={Missing Data, Imputation, and the Bootstrap}, volume={89}, ISSN={0162-1459, 1537-274X}, url={https://www.tandfonline.com/doi/full/10.1080/01621459.1994.10476768}, DOI={10.1080/01621459.1994.10476768}, number={426}, journal={Journal of the American Statistical Association}, author={Efron, Bradley}, year={1994}, month={6}, pages={463–475}, language={en} }

@book{buuren_flexible_2012,
	location = {Boca Raton, {FL}},
	title = {Flexible imputation of missing data},
	isbn = {9781439868249},
	series = {Chapman \& Hall/{CRC} interdisciplinary statistics series},
	abstract = {"Preface We are surrounded by missing data. Problems created by missing data in statistical analysis have long been swept under the carpet. These times are now slowly coming to an end. The array of techniques to deal with missing data has expanded considerably during the last decennia. This book is about one such method: multiple imputation. Multiple imputation is one of the great ideas in statistical science. The technique is simple, elegant and powerful. It is simple because it flls the holes in the data with plausible values. It is elegant because the uncertainty about the unknown data is coded in the data itself. And it is powerful because it can solve 'other' problems that are actually missing data problems in disguise. Over the last 20 years, I have applied multiple imputation in a wide variety of projects. I believe the time is ripe for multiple imputation to enter mainstream statistics. Computers and software are now potent enough to do the required calculations with little e ort. What is still missing is a book that explains the basic ideas, and that shows how these ideas can be put to practice. My hope is that this book can ll this gap. The text assumes familiarity with basic statistical concepts and multivariate methods. The book is intended for two audiences: - (bio)statisticians, epidemiologists and methodologists in the social and health sciences; - substantive researchers who do not call themselves statisticians, but who possess the necessary skills to understand the principles and to follow the recipes. In writing this text, I have tried to avoid mathematical and technical details as far as possible. Formula's are accompanied by a verbal statement that explains the formula in layman terms"--},
	pagetotal = {316},
	publisher = {{CRC} Press},
	author = {van Buuren, Stef},
	date = {2012},
	keywords = {Missing observations (Statistics), Multiple imputation (Statistics), Multivariate analysis},
}

@article{chen_jackknife_2001,
	title = {Jackknife Variance Estimation for Nearest-Neighbor Imputation},
	volume = {96},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214501750332839},
	doi = {10.1198/016214501750332839},
	abstract = {Nearest-neighbor imputation is a popular hot deck imputation method used to compensate for nonresponse in sample surveys. Although this method has a long history of application, the problem of variance estimation after nearest-neighbor imputation has not been fully investigated. Because nearest-neighbor imputation is a nonparametric method, a nonparametric variance estimation technique, such as the jackknife, is desired. We show that the naive jackknife that treats imputed values as observed data produces serious underestimation. We also show that Rao and Shao's adjusted jackknife, or the jackknife with each pseudoreplicate reimputed, which produces asymptotically unbiased and consistent jackknife variance estimators for other imputation methods (such as mean imputation, random hot deck imputation, and ratio or regression imputation), produces serious overestimation in the case of nearest-neighbor imputation. Two partially reimputed and a partially adjusted jackknife variance estimators are proposed and shown to be asymptotically unbiased and consistent. Some empirical results are provided to examine finite-sample properties of these jackknife variance estimators.},
	pages = {260--269},
	number = {453},
	journaltitle = {Journal of the American Statistical Association},
	author = {Chen, Jiahua and Shao, Jun},
	urldate = {2022-06-11},
	date = {2001-03-01},
	keywords = {Adjusted jackknife, Hot deck, Reimputation, Sample mean, Stratified sampling, Unbiasedness},
}

@article{giganti_multiple-imputation_2020,
	title = {Multiple-Imputation Variance Estimation in Studies With Missing or Misclassified Inclusion Criteria},
	volume = {189},
	issn = {0002-9262},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7705600/},
	doi = {10.1093/aje/kwaa153},
	abstract = {In observational studies using routinely collected data, a variable with a high level of missingness or misclassification may determine whether an observation is included in the analysis. In settings where inclusion criteria are assessed after imputation, the popular multiple-imputation variance estimator proposed by Rubin (“Rubin’s rules” ({RR})) is biased due to incompatibility between imputation and analysis models. While alternative approaches exist, most analysts are not familiar with them. Using partially validated data from a human immunodeficiency virus cohort, we illustrate the calculation of an imputation variance estimator proposed by Robins and Wang ({RW}) in a scenario where the study exclusion criteria are based on a variable that must be imputed. In this motivating example, the corresponding imputation variance estimate for the log odds was 29\% smaller using the {RW} estimator than using the {RR} estimator. We further compared these 2 variance estimators with a simulation study which showed that coverage probabilities of 95\% confidence intervals based on the {RR} estimator were too high and became worse as more observations were imputed and more subjects were excluded from the analysis. The {RW} imputation variance estimator performed much better and should be employed when there is incompatibility between imputation and analysis models. We provide analysis code to aid future analysts in implementing this method.},
	pages = {1628--1632},
	number = {12},
	journaltitle = {American Journal of Epidemiology},
	shortjournal = {Am J Epidemiol},
	author = {Giganti, Mark J and Shepherd, Bryan E},
	urldate = {2022-06-10},
	date = {2020-07-20},
	pmid = {32685964},
	pmcid = {PMC7705600},
}

@article{rao_jackknife_1992,
	title = {Jackknife Variance Estimation with Survey Data Under Hot Deck Imputation},
	volume = {79},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2337236},
	doi = {10.2307/2337236},
	abstract = {Hot deck imputation is commonly employed for item nonresponse in sample surveys. It is also a common practice to treat the imputed values as if they are true values, and then compute the variance estimates using standard formulae. This procedure, however, could lead to serious underestimation of the true variance, when the proportion of missing values for an item is appreciable. We propose a jackknife variance estimator for stratified multistage surveys which is obtained by first adjusting the imputed values for each pseudo-replicate and then applying the standard jackknife formula. The proposed jackknife variance estimator is shown to be consistent as the sample size increases, assuming equal response probabilities within imputation classes and using a particular hot deck imputation.},
	pages = {811--822},
	number = {4},
	journaltitle = {Biometrika},
	author = {Rao, J. N. K. and Shao, J.},
	urldate = {2022-06-10},
	date = {1992},
}

@article{bartlett_reference-based_2021,
	title = {Reference-Based Multiple Imputation—What is the Right Variance and How to Estimate It},
	issn = {1946-6315},
	url = {https://www.tandfonline.com/doi/full/10.1080/19466315.2021.1983455},
	doi = {10.1080/19466315.2021.1983455},
	pages = {1--9},
	journaltitle = {Statistics in Biopharmaceutical Research},
	shortjournal = {Statistics in Biopharmaceutical Research},
	author = {Bartlett, Jonathan W.},
	urldate = {2022-06-07},
	date = {2021-11-12},
	langid = {english},
}

@article{bartlett_bootstrap_2020,
	title = {Bootstrap inference for multiple imputation under uncongeniality and misspecification},
	volume = {29},
	issn = {0962-2802, 1477-0334},
	url = {http://journals.sagepub.com/doi/10.1177/0962280220932189},
	doi = {10.1177/0962280220932189},
	abstract = {Multiple imputation has become one of the most popular approaches for handling missing data in statistical analyses. Part of this success is due to Rubin’s simple combination rules. These give frequentist valid inferences when the imputation and analysis procedures are so-called congenial and the embedding model is correctly specified, but otherwise may not. Roughly speaking, congeniality corresponds to whether the imputation and analysis models make different assumptions about the data. In practice, imputation models and analysis procedures are often not congenial, such that tests may not have the correct size, and confidence interval coverage deviates from the advertised level. We examine a number of recent proposals which combine bootstrapping with multiple imputation and determine which are valid under uncongeniality and model misspecification. Imputation followed by bootstrapping generally does not result in valid variance estimates under uncongeniality or misspecification, whereas certain bootstrap followed by imputation methods do. We recommend a particular computationally efficient variant of bootstrapping followed by imputation.},
	pages = {3533--3546},
	number = {12},
	journaltitle = {Statistical Methods in Medical Research},
	shortjournal = {Stat Methods Med Res},
	author = {Bartlett, Jonathan W and Hughes, Rachael A},
	urldate = {2022-06-07},
	date = {2020-12},
	langid = {english},
}

@article{xie_dissecting_2016,
	title = {Dissecting multiple imputation from a multi-phase inference perspective: what happens when God’s, imputer’s and analyst’s models are uncongenial?},
	issn = {10170405},
	url = {http://www3.stat.sinica.edu.tw/statistica/J27N4/J27N41/J27N41-10.html},
	doi = {10.5705/ss.2014.067},
	shorttitle = {Dissecting multiple imputation from a multi-phase inference perspective},
	journaltitle = {Statistica Sinica},
	shortjournal = {{STAT} {SINICA}},
	author = {Xie, Xianchao and Meng, Xiao-Li},
	urldate = {2022-06-07},
	date = {2016},
}

@article{meng_multiple-imputation_1994,
	title = {Multiple-Imputation Inferences with Uncongenial Sources of Input},
	volume = {9},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-9/issue-4/Multiple-Imputation-Inferences-with-Uncongenial-Sources-of-Input/10.1214/ss/1177010269.full},
	doi = {10.1214/ss/1177010269},
	abstract = {Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications. Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is "uncongenial" to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.},
	pages = {538--558},
	number = {4},
	journaltitle = {Statistical Science},
	author = {Meng, Xiao-Li},
	urldate = {2022-06-07},
	date = {1994-11},
	keywords = {Congeniality, Normalizing constants, Randomization, importance sampling, incomplete data, missing data, nonresponse, public-use data file, self-efficiency},
}

@inproceedings{rao2000variance,
  title={Variance estimation in the presence of imputation for missing data},
  author={Rao, JNK},
  booktitle={Proceedings of the Second International Conference on Establishment Surveys},
  pages={599--608},
  year={2000}
}

@inproceedings{arcaro2001variance,
  title={Variance estimation in the presence of imputation},
  author={Arcaro, Charlie and Yung, Wesley},
  booktitle={SSC Annual Meeting, Proceedings of the Survey Method Section},
  pages={75--80},
  year={2001}
}

@article{lee1995variance,
  title={Variance estimation in the presence of imputed data for the generalized estimation system},
  author={Lee, H and Rancourt, E and Sarndal, CE},
  journal={Proc. of the American Statist. Assoc.(Social Survey Research Methods Section)},
  pages={384--389},
  year={1995}
}


@article{zhong_jackknife_2014,
	title = {Jackknife empirical likelihood inference with regression imputation and survey data},
	volume = {129},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X14000931},
	doi = {10.1016/j.jmva.2014.04.010},
	abstract = {We propose jackknife empirical likelihood ({EL}) methods for constructing confidence intervals of mean with regression imputation that allows ignorable or nonignorable missingness. The confidence interval is constructed based on the adjusted jackknife pseudo-values (Rao and Shao, 1992). The proposed {EL} ratios evaluated at the true value converge to the standard chi-square distribution under both missing mechanisms for simple random sampling. Thus the {EL} can be applied to construct a Wilks type confidence interval without any secondary estimation. We then extend the proposed method to accommodate Poisson sampling design in survey sampling. The proposed methods are compared with some existing methods in simulation studies. We also apply the proposed method to an Italy household income panel survey data set.},
	pages = {193--205},
	journaltitle = {Journal of Multivariate Analysis},
	shortjournal = {Journal of Multivariate Analysis},
	author = {Zhong, Ping-Shou and Chen, Sixia},
	urldate = {2022-06-12},
	date = {2014-08-01},
	langid = {english},
	keywords = {Kernel smoothing, Missing at random, Nonignorable missing, Response mechanism, Wilks’ theorem},
}

@article{shao_bootstrap_1996,
	title = {Bootstrap for Imputed Survey Data},
	volume = {91},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476997},
	doi = {10.1080/01621459.1996.10476997},
	abstract = {Most surveys use imputation to compensate for missing data. However, treating the imputed data set as the complete data set and directly applying existing methods (e.g., the linearization, the jackknife, and the bootstrap) for variance estimation and/or statistical inference does not produce valid results, because these methods do not account for the effect of missing data and/or imputation. In this article we show that correct bootstrap estimates can be obtained by imitating the process of imputing the original data set in the bootstrap resampling; that is, by imputing the bootstrap data sets in exactly the same way that the original data set is imputed. The proposed bootstrap is asymptotically valid irrespective of the sampling design, the imputation method, or the type of statistic used in inference. This enables us to use a unified method in a variety of problems, and in fact this is the only method that works without any restriction on the sampling design, the imputation method, or the type of statistic.},
	pages = {1278--1288},
	number = {435},
	journaltitle = {Journal of the American Statistical Association},
	author = {Shao, Jun and Sitter, Randy R.},
	urldate = {2022-06-12},
	date = {1996-09-01},
	keywords = {Distribution and variance estimation, Item nonresponse, Multistage sampling, Random imputation, Ratio and regression imputation, Uniform response},
}

@article{eddings_diagnostics_2012,
	title = {Diagnostics for Multiple Imputation in Stata},
	volume = {12},
	issn = {1536-867X, 1536-8734},
	url = {http://journals.sagepub.com/doi/10.1177/1536867X1201200301},
	doi = {10.1177/1536867X1201200301},
	abstract = {Our new command midiagplots makes diagnostic plots for multiple imputations created by mi impute. The plots compare the distribution of the imputed values with that of the observed values so that problems with the imputation model can be corrected before the imputed data are analyzed. We include an example and suggest extensions to other diagnostics.},
	pages = {353--367},
	number = {3},
	journaltitle = {The Stata Journal: Promoting communications on statistics and Stata},
	shortjournal = {The Stata Journal},
	author = {Eddings, Wesley and Marchenko, Yulia},
	urldate = {2022-06-12},
	date = {2012-09},
	langid = {english},
}

@article{efron_missing_1994,
	title = {Missing Data, Imputation, and the Bootstrap},
	volume = {89},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.1994.10476768},
	doi = {10.1080/01621459.1994.10476768},
	abstract = {Missing data refers to a class of problems made difficult by the absence of some portions of a familiar data structure. For example, a regression problem might have some missing values in the predictor vectors. This article concerns nonparametric approaches to assessing the accuracy of an estimator in a missing data situation. Three main topics are discussed: bootstrap methods for missing data, these methods' relationship to the theory of multiple imputation, and computationally efficient ways of executing them. The simplest form of nonparametric bootstrap confidence interval turns out to give convenient and accurate answers. There are interesting practical and theoretical differences between bootstrap methods and the multiple imputation approach, as well as some useful similarities.},
	pages = {463--475},
	number = {426},
	journaltitle = {Journal of the American Statistical Association},
	author = {Efron, Bradley},
	urldate = {2022-06-12},
	date = {1994-06-01},
	keywords = {Bayesian bootstrap, Bootstrap confidence intervals, Data augmentation, Ignorable nonresponse, Nonparametric {MLE}},
}

@article{buuren_mice_2011,
	title = {mice: Multivariate Imputation by Chained Equations in R},
	volume = {45},
	rights = {Copyright (c) 2009 Stef van Buuren, Karin Groothuis-Oudshoorn},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v045.i03},
	doi = {10.18637/jss.v045.i03},
	shorttitle = {mice},
	abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-{PLUS} library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In mice, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
	pages = {1--67},
	journaltitle = {Journal of Statistical Software},
	author = {Buuren, Stef van and Groothuis-Oudshoorn, Karin},
	urldate = {2022-06-12},
	date = {2011-12-12},
	langid = {english},
}

@article{schomaker_bootstrap_2018,
	title = {Bootstrap Inference When Using Multiple Imputation},
	volume = {37},
	issn = {0277-6715},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5986623/},
	doi = {10.1002/sim.7654},
	abstract = {Many modern estimators require bootstrapping to calculate confidence intervals because either no analytic standard error is available or the distribution of the parameter of interest is non-symmetric. It remains however unclear how to obtain valid bootstrap inference when dealing with multiple imputation to address missing data. We present four methods which are intuitively appealing, easy to implement, and combine bootstrap estimation with multiple imputation. We show that three of the four approaches yield valid inference, but that the performance of the methods varies with respect to the number of imputed data sets and the extent of missingness. Simulation studies reveal the behavior of our approaches in finite samples. A topical analysis from {HIV} treatment research, which determines the optimal timing of antiretroviral treatment initiation in young children, demonstrates the practical implications of the four methods in a sophisticated and realistic setting. This analysis suffers from missing data and uses the g-formula for inference, a method for which no standard errors are available.},
	pages = {2252--2266},
	number = {14},
	journaltitle = {Statistics in medicine},
	shortjournal = {Stat Med},
	author = {Schomaker, M. and Heumann, C.},
	urldate = {2022-06-12},
	date = {2018-06-30},
	pmid = {29682776},
	pmcid = {PMC5986623},
}

@article{von_Hippel_2020, title={How many imputations do you need? A two-stage calculation using a quadratic rule}, volume={49}, ISSN={0049-1241, 1552-8294}, url={http://journals.sagepub.com/doi/10.1177/0049124117747303}, DOI={10.1177/0049124117747303}, abstractNote={When using multiple imputation, users often want to know how many imputations they need. An old answer is that 2–10 imputations usually suffice, but this recommendation only addresses the efficiency of point estimates. You may need more imputations if, in addition to efficient point estimates, you also want standard error (SE) estimates that would not change (much) if you imputed the data again. For replicable SE estimates, the required number of imputations increases quadratically with the fraction of missing information (not linearly, as previous studies have suggested). I recommend a two-stage procedure in which you conduct a pilot analysis using a small-to-moderate number of imputations, then use the results to calculate the number of imputations that are needed for a final analysis whose SE estimates will have the desired level of replicability. I implement the two-stage procedure using a new SAS macro called \%mi_combine and a new Stata command called how_many_imputations.}, number={3}, journal={Sociological Methods \& Research}, author={von Hippel, Paul T.}, year={2020}, month={8}, pages={699–718}, language={en} }