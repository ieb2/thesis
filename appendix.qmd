# Appendix {.unnumbered}

## Notation 
  
  * $N$ is the total number of units in the finite population being targeted.  
  * $X$ is an $N \times q$ matrix of fully observed covariates.  
  * $Y$ is an $N \times p$ matrix of partially observed outcome variables.  
  * $R$ is an $N \times p$ matrix of response indicators (i.e., $R_{ij} = 1$ if the response on $Y_{ij}$ is obtained and $R_{ij} = 0$ otherwise.)  
  * $Q$ is an unknown quantity of interest to the analyst. 
  * $Z_c = \{X, Y_{inc}\}$ is the complete data. 
  * $Z_o = \{X, Y_{obs}, R_{inc}\}$ is the incomplete (i.e., observed) data. 
  * The analyst's complete-data procedure is summarized by $\mathscr{P}_{com} = [\hat{Q}(X, Y_{inc}), U(X, Y_{inc})]$,   where $\hat{Q}(X, Y_{inc})$ is an estimator of $Q$ with associated variance $U(X, Y_{inc}.)$ 
  * $R$ is not a part of $\mathscr{P}_{com}$, as the missing at random assumption implies that the response behavior itself carries no information about $Q$. 

## Formal Definition of Congeniality

In short, one may define congeniality as the imputer and analyst making different assumptions regarding the data. The following two-part formal definition of uncongeniality was proposed by Meng in 1994, and will be utilized in our research. Meeting the assumptions set forth in the following two definitions qualifies the imputation model as being congenial to the analysis model, or vice versa. 

### Definition 1
Let $E_f$ and $V_f$ denote posterior mean and variance with respect to $f$, respectively. A Bayesian model $f$ is said to be congenial to the analysis procedure $\mathscr{P} \equiv \{\mathscr{P}_{obs}; \mathscr{P}_{com}\}$ for given $Z_o$ if the following hold: 

  * The posterior mean and variance of $\theta$ under $f$ given the incomplete data are asymptotically the same as the estimate and variance from the analyst's incomplete-data procedure $\mathscr{P}_{obs}$, that is, 
    \begin{equation}
        [\hat{\theta}(Z_o), U(Z_o)] \simeq [E_f[\theta | Z_o], V_f[\theta | Z_o]]
    \end{equation}
    
  * The posterior mean and variance of $\theta$ under $f$ given the complete data are asymptotically the same as the estimate and variance from the analyst's complete-data procedure $\mathscr{P}_{com}$, that is, 
    \begin{equation}
         [\hat{\theta}(Z_c), U(Z_c)] \simeq [E_f[\theta | Z_c], V_f[\theta | Z_c]]
    \end{equation}
    
    for any possible $Y_{inc} = (Y_{obs}, Y_{miss})$ with $Y_{obs}$ conditioned upon.

If the foregoing conditions are met, $f$ is said to be second-moment congenial to $\mathscr{P}$.

### Definition 2 

The analysis procedure $\mathscr{P}$ is said to be congenial to the imputation model $g(Y_{miss}|Z_o, A)$ where $A$ represents possible additional data the imputer has access to, if one can find an $f$ such that (**i**) $f$ is congenial to $\mathscr{P}$ and (**ii**) the posterior predictive density for $Y_{miss}$ derived under $f$ is identical to the imputation model $f(Y_{miss}|Z_o) = g(Y_{miss}|Z_o, A) \ \forall \ Y_{miss}$.

## Raw Data 
```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
read_csv("final_sim_data_agg.csv") %>%
  select(-"...1") %>%
  as_tibble()
```

## Code 

Please see our publicly available GitHub [repo](https://github.com/ieb2/jackknife_var_est.git) for the simulation code. Otherwise, the code utilized through the paper has been included below. 

```{r, eval=FALSE}
# Aggregated code here for organization, make it available in an appendix.
library(tidyverse)
library(ggpubr)

combined_results <- read_csv("final_sim_data_agg.csv") %>%
  select(-"...1")

benchmark_res <- read.csv("benchmark_detailed_res.csv") %>%
  select(-X) %>%
  group_by(expr) %>%
  summarise("Mean Time (seconds/iteration)" = mean(time)/1e6/100, 
            "Median Time (seconds/iteration)" = median(time)/1e6/100, 
            "SD of Time (seconds/iteration)" = sd(time)/1e6/100) %>%
  rename("Method" = expr)

benchmark_res_ord <- benchmark_res[c(2,1,3),] 

jackknife_coverage <- 
  round((sum(combined_results$true_var < combined_results$UB &
               combined_results$true_var > combined_results$LB) / nrow(combined_results))*100,2)

rubin_coverage <- 
  round((sum(combined_results$true_var < combined_results$UB_rub &
               combined_results$true_var > combined_results$LB_rub) / nrow(combined_results))*100,2)

boot_coverage <- 
  round((sum(combined_results$true_var < combined_results$UB_boot &
               combined_results$true_var > combined_results$LB_boot) / nrow(combined_results))*100,2)

ci_sum <- tibble(
  "Method" = c("Jackknife", "Bootstrap", "Rubin's Rules"), 
  
  "Coverage Probability" = c(jackknife_coverage, rubin_coverage, boot_coverage), 
  
  "Median C.I. Width" = c(median(combined_results$jackknife_width), median(combined_results$boot_width), median(combined_results$rubin_width)), 
  
  "Mean C.I. Width" = c(mean(combined_results$jackknife_width), mean(combined_results$boot_width), mean(combined_results$rubin_width)), 
  
  "SD of C.I. Width" = c(sd(combined_results$jackknife_width), sd(combined_results$boot_width), sd(combined_results$rubin_width))
)

point_estimate_sum <- tibble(
  "Method" = c("Jackknife", "Bootstrap", "Rubin's Rules"), 
  
  "Median Point Estimates " = c(median(combined_results$point_estimate), median(combined_results$point_estimate_boot), median(combined_results$point_estimate_rub)), 
  
  "Mean Point Estimates" = c(mean(combined_results$point_estimate), mean(combined_results$point_estimate_boot), mean(combined_results$point_estimate_rub)), 
  
  "SD of Point Estimates" = c(sd(combined_results$point_estimate), sd(combined_results$point_estimate_boot), sd(combined_results$point_estimate_rub))
)

set.seed(234)
jackk <- combined_results %>%
  sample_n(1e2, replace = FALSE) %>%
  mutate(covers = ifelse(UB > true_var & true_var > LB, "Covers", "Does Not Cover")) %>%
  ggplot(., aes(x = 1:100)) + 
  geom_errorbar(aes(ymin = LB, ymax = UB, color = covers)) + 
  theme_bw() + 
  #coord_flip() + 
  labs(
    x = latex2exp::TeX("$i^{th} iteration"), 
    y = "C.I."
  ) + 
  geom_hline(yintercept = combined_results$true_var) + 
  scale_color_manual(name = NULL, values=c("#999999", "#FF0000")) + 
  ggtitle("Jackknife Estimator") + 
  xlim(c(0,100)) + 
  ylim(c(-2.5,5))

set.seed(24123)
boot <- combined_results %>%
  sample_n(1e2, replace = FALSE) %>%
  mutate(covers = ifelse(UB_boot > true_var & true_var > LB_boot, "Covers", "Does Not Cover")) %>%
  ggplot(., aes(x = 1:100)) + 
  geom_errorbar(aes(ymin = LB_boot, ymax = UB_boot, color = covers)) + 
  theme_bw() + 
  #coord_flip() + 
  labs(
    x = latex2exp::TeX("$i^{th} iteration"), 
    y = "C.I."
  ) + 
  geom_hline(yintercept = combined_results$true_var) + 
  scale_color_manual(name = NULL, values=c("#999999", "#FF0000")) + 
  ggtitle("Bootstrap Estimator") + 
  xlim(c(0,100)) + 
  ylim(c(-2.5,5))

set.seed(234)
rubin <- combined_results %>%
  sample_n(1e2, replace = FALSE) %>%
  mutate(covers = ifelse(UB_rub > true_var & true_var > LB_rub, "Covers", "Does Not Cover")) %>%
  ggplot(., aes(x = 1:100)) + 
  geom_errorbar(aes(ymin = LB_rub, ymax = UB_rub, color = covers)) + 
  theme_bw() + 
  #coord_flip() + 
  labs(
    x = latex2exp::TeX("$i^{th} iteration"), 
    y = "C.I."
  ) + 
  geom_hline(yintercept = combined_results$true_var) + 
  scale_color_manual(name = NULL, values=c("#999999", "#FF0000")) + 
  ggtitle("Rubin's Rules") + 
  xlim(c(0,100)) + 
  ylim(c(-2.5,5))

point_dist <- reshape2::melt(combined_results[c("point_estimate_rub", "point_estimate", "point_estimate_boot")]) %>%
  ggplot(data = .,
         aes(x = value - 2, fill = variable, color = variable)) + 
  geom_density(aes(y = ..density..), alpha = 0.25) + 
  theme(axis.title.y = element_blank(), 
        panel.spacing=unit(1.5,"lines")) + 
  theme_bw() + 
  theme(axis.title.y = element_blank(), 
        panel.spacing=unit(1.5,"lines"), 
        strip.text = element_text(
          size = 9)) + 
  labs(
    x = latex2exp::TeX("$\\widehat{\\beta_1} - \\beta_1")
  ) + 
  scale_fill_discrete(name = "Method", labels = c("Rubin's Rules", "Jackknife", "Bootstrap")) + 
  scale_color_discrete(name = "Method", labels = c("Rubin's Rules", "Jackknife", "Bootstrap"))

benchmark_plot <- read.csv("benchmark_detailed_res.csv") %>%
  select(-X) %>%
  ggplot(., aes(x = expr, y = time/1e6/100)) + 
  geom_violin() + 
  scale_y_log10() + 
  theme_bw() + 
  ggtitle("Benchmark for Methods Examined") + 
  coord_flip() + 
  labs(
    y = "Time (seconds/iteration)"
  ) + 
  theme(axis.title.y = element_blank())
```
