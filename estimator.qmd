# The Proposed Jackknife Estimator

A pseudocode overview of the jackknife estimator proposed may be seen in the following figure.

![A pseudocode depiction of the proposed estimator.](Algorithm_for_jackknife_estimator.jpg){fig-align="center"}

Briefly, the algorithm begins by obtaining $j$ jackknife subsamples from the observed dataset with missing observations. Thereafter, each of the $j$ subsamples are imputed $m$ times, resulting in a total of $j \times m$ complete datasets. Subsequently, the analysis model of interest to estimate $\theta$ is applied to each of the completed datasets to produce $\hat{\theta^*}_1, \hat{\theta^*}_2, \hat{\theta^*}_3, \dots, \hat{\theta^*}_{n \choose d}$. Finally, the vector of estimates is trimmed such that values outside of the $\alpha^{th}$ and $1-\alpha^{th}$ percentiles are excluded from the determination of the point estimate and bounds of the confidence interval.

As part of the algorithm, researchers must choose values $d$ and $j$, which will be context-dependent quantities. Ideally, a $d$ value which satisfies $\frac{\sqrt{n}}{d} \rightarrow 0$ will provide asymptotically unbiased estimates even for non-smooth statistics. Rewriting the foregoing condition for $d$:

$$
\begin{align}
  &\frac{\sqrt{n}}{d} \rightarrow \ 0 \\ 
  &\implies d >> \sqrt{n} \\ 
  &\text{Since} \ d < n \\
  &\implies n > d >> \sqrt{n}
\end{align}
$$

It is evident that $d$ should take on some value between $n$ and $\sqrt{n}$ with $d$ being closer to $n$, particulary for non-smooth statistics. At any rate, $j = \binom{n}{d}$ will likely be a value that is not computationally feasible to obtain. Again, the number of subsamples required, $j$, can be limited to yield the estimator more accessible. The choice of $j$ will be a multifaceted decision, where, if possible, greater values are preferred. Ideally, a small pilot study may be performed with a range of $j$ values to determine values of $j$ for which estimates begin to converge.

## Theoretical Basis of the Proposed Estimator 

Several contrasts may be noted compared to similar estimators proposed for variance estimation in multiply imputed datasets. The use of jackknife subsamples as opposed to bootstrap subsamples (i.e., subsamples constructed without replacement vs. those constructed with replacement), imputing the jackknifed subsamples, as opposed to jackknifing the imputed datasets and trimming the point estimates prior to obtaining the point estimate and the bounds of the percentile confidence interval. Each of the aforementioned contrasts will be addressed in the following subsections. 

### Jackknife vs. Bootstrap Subsamples 

### Order of Impuation and Subsapling 

### Generation of the Confidence Interval