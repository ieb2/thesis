# The Proposed Jackknife Estimator

A pseudocode overview of the jackknife estimator proposed may be seen in the following figure.

![A pseudocode depiction of the proposed estimator.](Algorithm_for_jackknife_estimator.jpg){fig-align="center"}

In short, the algorithm performs the following: a dataset with missing observations is imputed $m$ times to obtain numerous complete versions of the observed dataset. Thereafter, jackknife subsamples of size $\binom{m}{m-1}$ are generated by systematically leaving one of the imputed datasets out of the set. After this time, the observations within the previously constructed jackknife subsamples are subsampled again using a delete-*d*-jackknife algorithm. The analysis model of interest is applied to each of the subsamples obtained via the delete-*d*-jackknife algorithm and is then pooled with the estimates obtained from all elements of the outer jackknife subsampling scheme. Finally, the point estimate is found by getting the mean of the vector of point estimates and the confidence interval by utilizing the percentile method. Although not utilized here, a possible modification to the algorithm includes converting the outer jackknife subsampling scheme into a delete-*d*-jackknife , which may yield improved estimates at the cost of increased computational time.  

As part of the algorithm, researchers must choose values $k$ and $j$, which will be context-dependent quantities. Ideally, a $k$ value which satisfies $\frac{\sqrt{n}}{k} \rightarrow 0$ will provide asymptotically unbiased estimates even for non-smooth statistics. Rewriting the foregoing condition for $k$:

$$
\begin{align}
  &\frac{\sqrt{n}}{k} \rightarrow \ 0 \\ 
  &\implies k >> \sqrt{n} \\ 
  &\text{Since} \ k \leq n \\
  &\implies n \geq k >> \sqrt{n}
\end{align}
$$

It is evident that $k$ should take on some value between $n$ and $\sqrt{n}$ with $k$ being closer to $n$, particulary for non-smooth statistics. At any rate, $j = \binom{n}{k}$ will likely be a value that is not computationally feasible to obtain. Again, the number of subsamples required, $j$, can be limited to yield the estimator more accessible. The choice of $j$ will be a multifaceted decision, where, if possible, greater values are preferred. Ideally, a small pilot study may be performed with a range of $j$ values to determine values of $j$ for which estimates begin to converge. 

As for the outer jackknife, usually, lower values of $m$ provide satisfactory estimates of the between-imputation variance. Similar to the choice of $j$ and $k$, for non-smooth statistics with high uncertainty, higher values of $m$ are preferred. Depending on the context of the study being performed, researchers may also consider utilizing low $m$ and adding noise to the data to properly capture the uncertainity. 
