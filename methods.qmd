# Methods 

For the proposed Monte Carlo simulation, $N$ = 10,000 datasets were generated with the following characteristics: A response variable, $Y$, where 30% of the observations were missing at random (MAR)[^background-3], and an $n \times q$ matrix of fully observed covariates, where $n = 50$ was the sample size, and $q = 3$ the number of covariates. 

[^background-3]: Please see the appendix for a detailed overview of mechanisms of missingness, and their implications.

Formally 

$$
\begin{bmatrix} V1 \\V2 \\ V3 \end{bmatrix} \sim N\left(\begin{bmatrix} 1\\ 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 & 0.5 & 0.5 \\
0.5 & 1 & 0.5 \\
0.5 & 0.5 & 1
\end{bmatrix}\right)
$$
With 

$$
\beta_{V_1} = 2 ; \  \beta_{V_2} = 5 ; \ \beta_{V_3} = 8
$$
The outcome variable was simulated such that: 

$$
Y = V_1 + V_2 + V_3 + \epsilon
$$
Where 

$$
\epsilon \sim N(\mu = 0, \sigma \propto V_2)
$$
Thus, data were simulated with heteroskedastic errors, which yields the imputation and analysis models congenial, yet misspecified. 

Formally, the analysis model of interest was 

$$
\widehat{Y} \sim \widehat{\beta}_{V_1} + \widehat{\beta}_{V_2} + \widehat{\beta}_{V_3} 
$$
And the imputation model was 

$$
\widehat{Y}_{\text{mis}} \sim \widehat{\beta}_{V_1} + \widehat{\beta}_{V_2} + \widehat{\beta}_{V_3}
$$

Where the imputation method of choice was predictive mean matching (PMM). 

All generated datasets were analysed using three approaches: 

  * **Bootstrap then multiply impute:** The observed dataset with missing observations was initially bootstrapped $n = 200$ times. Thereafter, each of the bootstrap samples were imputed $m = 2$ times, with a maximum of $maxit = 5$ iterations. The mean of the bootstrap estimates served as the final point estimate, and a 95% confidence interval was generated through the percentile method, where the $\alpha/2^{th}$ and $1-(\alpha/2)^{th}$ percentiles were the lower and upper bounds, respectively. The R package, *bootImpute* was utilized for this process. 

  * **Multiply impute then use Rubin's rules:** The observed dataset with missing observations was imputed $m = 10$ times, with a maximum of $maxit = 5$ iterations. The point estimate, as well as the confidence interval was obtained through the following rules proposed by Donald Rubin. The *mice* package was utilized for this process. 

\begin{align}
\bar{\theta} &= \frac{1}{m}\left (\sum_{i=1}^m{\theta_i}\right ) \\ 
V_{\text{within}} &= \frac{1}{m} \sum_{i=1}^m{SE_i^2} \\ 
V_{\text{between}} &= \frac{\sum_{i=1}^m (\theta_i - \overline{\theta})^2}{m-1} \\ 
V_{\text{total}} &= V_W + V_B + \frac{V_B}{m} \\ 
\bar{\theta} &\pm t_{df,1-\alpha/2} * \sqrt{V_{\text{total}}} 
\end{align}

  * **Jackknife then multiply impute:** Although a more detailed overview of the proposed jackknife estimator is provided in @sec-est, briefly, the observed dataset was jackknifed to obtain $j = 200$ samples, each of which were imputed $m = 2$ times, with a maximum of $maxit = 5$ iterations. The mean of the jackknife estimates served as the final point estimate, and a 95% confidence interval was generated through the percentile method, where the $\alpha/2^{th}$ and $1-(\alpha/2)^{th}$ percentiles were the lower and upper bounds, respectively. 

Thereafter, the methods examined were compared concerning their point estimates, confidence intervals, and computational expense. 

All analyses were be conducted using R 4.2.1 (Funny-Looking Kid). 