# Methods 

For the proposed Monte Carlo simulation, $N$ = 10,000 datasets will be generated with the following characteristics: A response variable, $Y$, with $p_{miss} = 0.3$ proportion of missing observations, where the mechanism of missingness is missing at random (MAR), and an $n \times q$ matrix of fully observed covariates, where $n = 50$ is the sample size, and $q = 3$ is the number of covariates.  

Formally 

$$
\begin{bmatrix} V1 \\V2 \\ V3 \end{bmatrix} \sim N\left(\begin{bmatrix} 1\\ 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 & 0.5 & 0.5 \\
0.5 & 1 & 0.5 \\
0.5 & 0.5 & 1
\end{bmatrix}\right)
$$
With 

$$
\beta_{V_1} = 2 ; \beta_{V_2} = 5 ; \beta_{V_3} = 8
$$
The outcome variable was simulated such that: 

$$
Y = V1 + V2 + V3 + \epsilon
$$
Where 

$$
\epsilon \sim N(\mu = 0, sd \propto V2)
$$
Thus, data were simulated with heteroskedastic errors, which yields the imputation and analysis models congenial, yet misspecified. 

Formally, the analysis model of interest was 

$$
\hat{Y} \sim \hat{\beta_{V_1}} + \hat{\beta_{V_2}} + \hat{\beta_{V_3}} 
$$
And the imputation model was 

$$
\hat{Y_{\text{mis}}} \sim \hat{\beta_{V_1}} + \hat{\beta_{V_2}} + \hat{\beta_{V_3}} 
$$

Where the imputation method of choice was predictive mean matching (PMM). 

All generated datasets will be analysed using three approaches: 
  * Bootstrap then multiply impute: The observed dataset with missing observations will initially be bootstrapped $n = 200$ times. Thereafter, each of the bootstrap samples will be imputed $m = 2$ times, with a maximum of $maxit = 5$ iterations. The mean of the bootstrap estimates will serve as the final point estimate, and a 95% confidence interval will be generated through the percentile method, where the $\alpha/2^{th}$ and $1-(\alpha/2)^{th}$ percentiles will be the lower and upper bounds, respectively. The R package, *bootImpute* will be utilized for this process. 

  * Multiply impute then use Rubin's rules: The observed dataset with missing observations will be imputed $m = 10$ times, with a maximum of $maxit = 5$ iterations. The point estimate, as well as the confidence interval will be obtained through the following rules proposed by Donald Rubin. The *mice* package will be utilized for this process. 

\begin{align}
\bar{\theta} &= \frac{1}{m}\left (\sum_{i=1}^m{\theta_i}\right ) \\ 

V_{within} &= \frac{1}{m} \sum_{i=1}^m{SE_i^2} \\ 

V_{between} &= \frac{\sum_{i=1}^m (\theta_i - \overline{\theta})^2}{m-1} \\ 

V_{total} &= V_W + V_B + \frac{V_B}{m} \\ 

\bar{\theta} &\pm t_{df,1-\alpha/2} * \sqrt{V_{total}} \\ 
\end{align}

* Jackknife then multiply impute: Altough a more detailed overview of the proposed jackknife estimator is provided in @sec-est, briefly, the observed dataset will be jackknifed to obtain $j = 200$ samples, each of which will be imputed $m = 2$ times, with a maximum of $maxit = 5$ iterations. The mean of the jackknife estimates will serve as the final point estimate, and a 95% confidence interval will be generated through the percentile method, where the $\alpha/2^{th}$ and $1-(\alpha/2)^{th}$ percentiles will be the lower and upper bounds, respectively. 

Thereafter, the methods examined will be compared concerning their point estimates, confidence intervals, and computational expense. 

All analyses will be conducted using \texttt{R} 4.2.1 (Funny-Looking Kid). 